{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXxlR3kDG5YF0h+TnxquS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avkornaev/Sleep_Stages/blob/main/SleepStages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Instal Libraries"
      ],
      "metadata": {
        "id": "ItfC-WeXWArx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning clearml"
      ],
      "metadata": {
        "id": "VCAdnjnhvDVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1. Imports & Environment Setup\n",
        "# ---------------------------\n",
        "import os\n",
        "import shutil  # Add this line\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "from clearml import Task\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt  # Add this\n",
        "import seaborn as sns            # Add this\n",
        "\n",
        "import torchaudio  # For audio-specific augmentations\n",
        "import random\n",
        "from torch.utils.data import WeightedRandomSampler"
      ],
      "metadata": {
        "id": "NTSqV5Uzvquh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set and Initialize"
      ],
      "metadata": {
        "id": "glM38azgWIW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up reproducibility\n",
        "SEED = 42\n",
        "pl.seed_everything(SEED)\n",
        "torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "id": "pdQhGuy9vqsC",
        "outputId": "b008a8be-f370-4a39-f7ce-7923524a4bd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here to implement Step 2 of the logging instruction as it is shown below\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=ZP02U03C6V5ER4K9VWRNZT7EWA5ZTV\n",
        "%env CLEARML_API_SECRET_KEY=BtA5GXZufr6QGpaqhX1GSKPTvaCt56OLqaNqUGLNoxx2Ye8Ctwbui0Ln5OXVnzUgH4I"
      ],
      "metadata": {
        "id": "7z_tbUUHwCgR",
        "outputId": "732ad42d-06a3-4c4b-e1af-a39a883e72e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=ZP02U03C6V5ER4K9VWRNZT7EWA5ZTV\n",
            "env: CLEARML_API_SECRET_KEY=BtA5GXZufr6QGpaqhX1GSKPTvaCt56OLqaNqUGLNoxx2Ye8Ctwbui0Ln5OXVnzUgH4I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'Sleep_Stages' # dataset with the real-world noise\n",
        "#Clone the GitHub repository\n",
        "repo_url = \"https://github.com/avkornaev/Sleep_Stages\"  # Replace with your repo URL\n",
        "!git clone {repo_url}\n",
        "\n",
        "#Navigate to the data folder\n",
        "repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")  # Extract repo name\n",
        "data_dir = os.path.join(repo_name, \"data\")  # Replace \"data\" with your folder name\n",
        "# os.chdir(data_dir)  # Change working directory to the data folder\n",
        "\n",
        "# Verify the data directory\n",
        "if os.path.exists(data_dir):\n",
        "    print(f\"Data directory found: {data_dir}\")\n",
        "else:\n",
        "    print(f\"Data directory not found: {data_dir}\")"
      ],
      "metadata": {
        "id": "hyfkpXxwwPB2",
        "outputId": "deb2b59d-9659-400f-895e-af50da568d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sleep_Stages'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 59 (delta 26), reused 10 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (59/59), 7.31 MiB | 18.44 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "Data directory found: Sleep_Stages/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 2. Configuration Constants\n",
        "# ---------------------------\n",
        "CONFIG = {\n",
        "    \"data_dir\": \"Sleep_Stages/data\",\n",
        "    \"checkpoint_path\": \"saved_models/\",\n",
        "    \"batch_size\": 128,\n",
        "    \"window_size\": 600,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"max_epochs\": 2,\n",
        "    \"num_workers\": 2,\n",
        "    'focal_gamma': 2.0,  # Adjust based on imbalance severity\n",
        "    'augment_minority': True,\n",
        "    'batch_sampler': 'weighted'\n",
        "}\n",
        "MINORITY_CLASSES = ['R']"
      ],
      "metadata": {
        "id": "Xh9s5d7evqpg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funcitons"
      ],
      "metadata": {
        "id": "Qa84bISQWSI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 3. Data Preprocessing Block\n",
        "# ---------------------------\n",
        "def load_and_preprocess_data(directory: str) -> Tuple[pd.DataFrame, Dict[str, tuple], List[str], Dict[str, float]]:\n",
        "    \"\"\"Robust data loader with class weight calculation\"\"\"\n",
        "    # First pass: Load raw data and calculate global statistics\n",
        "    raw_data = defaultdict(list)\n",
        "    all_features = []\n",
        "    label_sets = defaultdict(set)\n",
        "\n",
        "    print(\"Loading raw data...\")\n",
        "    for filename in os.listdir(directory):\n",
        "        if not filename.startswith(\"Vol_\") or not filename.endswith(\".csv.gz\"):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(os.path.join(directory, filename),\n",
        "                           compression='gzip', skiprows=1, header=None,\n",
        "                           dtype={i: np.float32 for i in range(19)})\n",
        "            df.columns = [f\"feature_{i}\" for i in range(19)] + [\"label\"]\n",
        "            df['label'] = df['label'].str.strip().replace('', np.nan)\n",
        "            df = df.dropna(subset=['label'])\n",
        "\n",
        "            vol_id = filename.split(\"_\")[1].split(\".\")[0]\n",
        "            raw_data[vol_id].append(df)\n",
        "            label_sets[vol_id].update(df['label'].unique())\n",
        "            all_features.append(df.iloc[:, :-1].values)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Calculate global imputation values\n",
        "    global_mean = np.nanmean(np.concatenate(all_features), axis=0)\n",
        "    global_std = np.nanstd(np.concatenate(all_features), axis=0)\n",
        "\n",
        "    # Determine common labels across all volunteers\n",
        "    common_labels = set(label_sets[next(iter(label_sets))])\n",
        "    for labels in label_sets.values():\n",
        "        common_labels.intersection_update(labels)\n",
        "    common_labels = sorted(common_labels)\n",
        "    if not common_labels:\n",
        "        raise ValueError(\"No common labels found across all volunteers\")\n",
        "    print(f\"Common labels: {common_labels}\")\n",
        "\n",
        "    common_labels = sorted(common_labels)  # This should be defined earlier in the function\n",
        "    class_labels = common_labels  # Assign to correct variable name\n",
        "\n",
        "    # Second pass: Process each volunteer with validation\n",
        "    processed_data = {}\n",
        "    combined_dfs = []\n",
        "    label_to_id = {lbl: idx for idx, lbl in enumerate(common_labels)}\n",
        "\n",
        "    for vol_id, dfs in raw_data.items():\n",
        "        full_df = pd.concat(dfs)\n",
        "\n",
        "        # Data cleaning\n",
        "        full_df = full_df[full_df['label'].isin(common_labels)]\n",
        "        for i in range(19):\n",
        "            full_df[f'feature_{i}'] = full_df[f'feature_{i}'].fillna(global_mean[i])\n",
        "\n",
        "        # Validate label presence\n",
        "        present_labels = full_df['label'].unique()\n",
        "        if len(present_labels) != len(common_labels):\n",
        "            print(f\"Skipping {vol_id} - missing labels. Present: {present_labels}\")\n",
        "            continue\n",
        "\n",
        "        # Normalization\n",
        "        features = (full_df.iloc[:, :19].values - global_mean) / (global_std + 1e-8)\n",
        "        labels = full_df['label'].map(label_to_id).values\n",
        "\n",
        "        # Store processed data\n",
        "        processed_data[f\"Vol_{vol_id}\"] = (features.astype(np.float32), labels.astype(np.long))\n",
        "\n",
        "        # Create visualization dataframe\n",
        "        vis_df = pd.DataFrame(features, columns=[f\"feature_{i}\" for i in range(19)])\n",
        "        vis_df['volunteer'] = vol_id\n",
        "        vis_df['label'] = full_df['label'].values\n",
        "        combined_dfs.append(vis_df)\n",
        "\n",
        "    if not processed_data:\n",
        "        raise ValueError(\"No valid volunteers after preprocessing\")\n",
        "\n",
        "    combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
        "\n",
        "    # Calculate class weights\n",
        "    label_counts = combined_df['label'].value_counts()\n",
        "    class_weights = (1. / (label_counts / label_counts.sum())).to_dict()\n",
        "\n",
        "    return combined_df, processed_data, class_labels, class_weights\n",
        "\n",
        "    # # Calculate class weights\n",
        "    # label_counts = combined_df['label'].value_counts()\n",
        "    # class_weights = 1. / (label_counts / label_counts.sum())\n",
        "    # class_weights = class_weights.to_dict()\n",
        "\n",
        "    # return combined_df, processed_data, common_labels, class_weights\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Data Visualization\n",
        "# ---------------------------\n",
        "def visualize_data(combined_df: pd.DataFrame, common_labels: List[str]):\n",
        "    \"\"\"Generate comprehensive visualizations of the dataset\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.countplot(data=combined_df, x='volunteer', hue='label',\n",
        "                order=sorted(combined_df['volunteer'].unique()),\n",
        "                hue_order=common_labels)\n",
        "    plt.title('Label Distribution by Volunteer')\n",
        "    plt.xlabel('Volunteer ID')\n",
        "    plt.ylabel('Sample Count')\n",
        "    plt.legend(title='Sleep Stage')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance analysis\n",
        "    from sklearn.feature_selection import f_classif\n",
        "    X = combined_df[[c for c in combined_df.columns if c.startswith('feature_')]]\n",
        "    y = combined_df['label'].factorize()[0]\n",
        "\n",
        "    f_values, _ = f_classif(X, y)\n",
        "    feat_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'F-value': f_values\n",
        "    }).sort_values('F-value', ascending=False).head(10)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='F-value', y='Feature', data=feat_importance, palette='viridis')\n",
        "    plt.title('Top 10 Discriminative Features (ANOVA F-value)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Box plots for top 3 features\n",
        "    top_features = feat_importance.head(3)['Feature'].tolist()\n",
        "    for feat in top_features:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x='label', y=feat, data=combined_df,\n",
        "                  order=common_labels)\n",
        "        plt.title(f'Distribution of {feat} by Sleep Stage')\n",
        "        plt.xlabel('Sleep Stage')\n",
        "        plt.ylabel('Normalized Value')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Display sample data\n",
        "    print(\"\\nPreprocessed Data Sample:\")\n",
        "    display(combined_df.sample(5, random_state=SEED))\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Dataset & DataLoaders\n",
        "# ---------------------------\n",
        "class SleepDataset(Dataset):\n",
        "    def __init__(self, features: np.ndarray, labels: np.ndarray, window_size: int):\n",
        "        self.windows, self.labels = self._create_windows(features, labels, window_size)\n",
        "\n",
        "        # Initialize augmentation with random rate selection\n",
        "        self.augment = torch.nn.Sequential(\n",
        "            torchaudio.transforms.TimeStretch(\n",
        "                fixed_rate=random.choice([0.9, 1.0, 1.1])\n",
        "            ),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=15)\n",
        "        )\n",
        "\n",
        "    def _create_windows(self, features, labels, window_size):\n",
        "        windows = []\n",
        "        window_labels = []\n",
        "        for start in range(0, len(features) - window_size + 1, window_size//2):\n",
        "            window = features[start:start+window_size].T\n",
        "            label = labels[start+window_size-1]\n",
        "            windows.append(window)\n",
        "            window_labels.append(label)\n",
        "\n",
        "        # Optimized conversion\n",
        "        return (\n",
        "            torch.FloatTensor(np.stack(windows)),\n",
        "            torch.LongTensor(np.array(window_labels))\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window = self.windows[idx]\n",
        "        if self.labels[idx] in MINORITY_CLASSES:  # Apply to minority classes\n",
        "            window = self.augment(window)\n",
        "        return window, self.labels[idx]\n",
        "\n",
        "class SleepDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, processed_data: Dict[str, tuple], config: dict):\n",
        "        super().__init__()\n",
        "        self.data = processed_data\n",
        "        self.config = config\n",
        "        self.datasets = {}\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        for vol_id, (feats, lbls) in self.data.items():\n",
        "            self.datasets[vol_id] = SleepDataset(feats, lbls, self.config['window_size'])\n",
        "\n",
        "    def get_loso_splits(self, test_vol: str):\n",
        "        train_feats, train_lbls = [], []\n",
        "        for vol_id, dataset in self.datasets.items():\n",
        "            if vol_id != test_vol:\n",
        "                train_feats.append(dataset.windows)\n",
        "                train_lbls.append(dataset.labels)\n",
        "\n",
        "        train_ds = TensorDataset(torch.cat(train_feats), torch.cat(train_lbls))\n",
        "        test_ds = self.datasets[test_vol]\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, batch_size=self.config['batch_size'],\n",
        "                      shuffle=True, num_workers=self.config['num_workers']),\n",
        "            DataLoader(test_ds, batch_size=self.config['batch_size'],\n",
        "                     num_workers=self.config['num_workers'])\n",
        "        )\n",
        "\n",
        "# ---------------------------\n",
        "# Execution Flow\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    combined_df, processed_data, common_labels, class_weights = load_and_preprocess_data(CONFIG['data_dir'])\n",
        "\n",
        "    # Visualize dataset statistics\n",
        "    visualize_data(combined_df, common_labels)\n",
        "\n",
        "    # Initialize data module\n",
        "    datamodule = SleepDataModule(processed_data, CONFIG)\n",
        "    datamodule.setup()\n",
        "\n",
        "    # Example LOSO usage\n",
        "    test_subject = list(processed_data.keys())[0]\n",
        "    train_loader, val_loader = datamodule.get_loso_splits(test_subject)\n",
        "    print(f\"\\nLOSO Example: Training on {len(train_loader.dataset)} samples, \"\n",
        "          f\"Validating on {len(val_loader.dataset)} samples\")"
      ],
      "metadata": {
        "id": "i5rbezbgLO-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_titles = []"
      ],
      "metadata": {
        "id": "aDpJzDx9eBtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df"
      ],
      "metadata": {
        "id": "QIchyftsXtGV",
        "outputId": "2a2bde5b-c719-4805-f7c9-d5097b3a8258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
              "0              0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "1              0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "2              0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "3              0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "4              0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "...            ...        ...        ...        ...        ...        ...   \n",
              "1346020        0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "1346021        0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "1346022        0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "1346023        0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "1346024        0.0        0.0        0.0        0.0        0.0        0.0   \n",
              "\n",
              "         feature_6  feature_7  feature_8  feature_9  ...  feature_11  \\\n",
              "0              0.0        0.0        0.0   2.496306  ...    2.841512   \n",
              "1              0.0        0.0        0.0   2.426569  ...    2.841512   \n",
              "2              0.0        0.0        0.0   2.426569  ...    2.841512   \n",
              "3              0.0        0.0        0.0   2.426569  ...    2.841512   \n",
              "4              0.0        0.0        0.0   2.426569  ...    2.841512   \n",
              "...            ...        ...        ...        ...  ...         ...   \n",
              "1346020        0.0        0.0        0.0   0.000000  ...   -0.296927   \n",
              "1346021        0.0        0.0        0.0   0.000000  ...   -0.296927   \n",
              "1346022        0.0        0.0        0.0   0.000000  ...   -0.296927   \n",
              "1346023        0.0        0.0        0.0   0.000000  ...   -0.296927   \n",
              "1346024        0.0        0.0        0.0   0.000000  ...   -0.296927   \n",
              "\n",
              "         feature_12  feature_13  feature_14  feature_15  feature_16  \\\n",
              "0          1.172706   -0.272194   -0.990053   -0.763586    0.119754   \n",
              "1          1.172706   -0.272194   -1.033023   -0.763586    0.119754   \n",
              "2          1.172706   -0.272194   -1.033023   -0.763586    0.119754   \n",
              "3          1.172706   -0.272194   -1.033023   -0.763586    0.119754   \n",
              "4          1.172706   -0.272194   -1.033023   -0.763586    0.119754   \n",
              "...             ...         ...         ...         ...         ...   \n",
              "1346020   -0.459262   -0.296217    0.000000    0.000000    0.000000   \n",
              "1346021   -0.459262   -0.296217    0.000000    0.000000    0.000000   \n",
              "1346022   -0.459262   -0.296217    0.000000    0.000000    0.000000   \n",
              "1346023   -0.459262   -0.296217    0.000000    0.000000    0.000000   \n",
              "1346024   -0.459262   -0.296217    0.000000    0.000000    0.000000   \n",
              "\n",
              "         feature_17  feature_18  volunteer label  \n",
              "0         -0.706774   -0.328184         01    N2  \n",
              "1         -0.706774   -0.328184         01    N2  \n",
              "2         -0.706774   -0.328184         01    N2  \n",
              "3         -0.706774   -0.328184         01    N2  \n",
              "4         -0.706774   -0.328184         01    N2  \n",
              "...             ...         ...        ...   ...  \n",
              "1346020    0.000000    0.000000         03     R  \n",
              "1346021    0.000000    0.000000         03     R  \n",
              "1346022    0.000000    0.000000         03     R  \n",
              "1346023    0.000000    0.000000         03     R  \n",
              "1346024    0.000000    0.000000         03     R  \n",
              "\n",
              "[1346025 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6ac30b4-2dee-4463-b983-d2b599b21d70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_11</th>\n",
              "      <th>feature_12</th>\n",
              "      <th>feature_13</th>\n",
              "      <th>feature_14</th>\n",
              "      <th>feature_15</th>\n",
              "      <th>feature_16</th>\n",
              "      <th>feature_17</th>\n",
              "      <th>feature_18</th>\n",
              "      <th>volunteer</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.496306</td>\n",
              "      <td>...</td>\n",
              "      <td>2.841512</td>\n",
              "      <td>1.172706</td>\n",
              "      <td>-0.272194</td>\n",
              "      <td>-0.990053</td>\n",
              "      <td>-0.763586</td>\n",
              "      <td>0.119754</td>\n",
              "      <td>-0.706774</td>\n",
              "      <td>-0.328184</td>\n",
              "      <td>01</td>\n",
              "      <td>N2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.426569</td>\n",
              "      <td>...</td>\n",
              "      <td>2.841512</td>\n",
              "      <td>1.172706</td>\n",
              "      <td>-0.272194</td>\n",
              "      <td>-1.033023</td>\n",
              "      <td>-0.763586</td>\n",
              "      <td>0.119754</td>\n",
              "      <td>-0.706774</td>\n",
              "      <td>-0.328184</td>\n",
              "      <td>01</td>\n",
              "      <td>N2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.426569</td>\n",
              "      <td>...</td>\n",
              "      <td>2.841512</td>\n",
              "      <td>1.172706</td>\n",
              "      <td>-0.272194</td>\n",
              "      <td>-1.033023</td>\n",
              "      <td>-0.763586</td>\n",
              "      <td>0.119754</td>\n",
              "      <td>-0.706774</td>\n",
              "      <td>-0.328184</td>\n",
              "      <td>01</td>\n",
              "      <td>N2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.426569</td>\n",
              "      <td>...</td>\n",
              "      <td>2.841512</td>\n",
              "      <td>1.172706</td>\n",
              "      <td>-0.272194</td>\n",
              "      <td>-1.033023</td>\n",
              "      <td>-0.763586</td>\n",
              "      <td>0.119754</td>\n",
              "      <td>-0.706774</td>\n",
              "      <td>-0.328184</td>\n",
              "      <td>01</td>\n",
              "      <td>N2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.426569</td>\n",
              "      <td>...</td>\n",
              "      <td>2.841512</td>\n",
              "      <td>1.172706</td>\n",
              "      <td>-0.272194</td>\n",
              "      <td>-1.033023</td>\n",
              "      <td>-0.763586</td>\n",
              "      <td>0.119754</td>\n",
              "      <td>-0.706774</td>\n",
              "      <td>-0.328184</td>\n",
              "      <td>01</td>\n",
              "      <td>N2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1346020</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.296927</td>\n",
              "      <td>-0.459262</td>\n",
              "      <td>-0.296217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>03</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1346021</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.296927</td>\n",
              "      <td>-0.459262</td>\n",
              "      <td>-0.296217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>03</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1346022</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.296927</td>\n",
              "      <td>-0.459262</td>\n",
              "      <td>-0.296217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>03</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1346023</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.296927</td>\n",
              "      <td>-0.459262</td>\n",
              "      <td>-0.296217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>03</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1346024</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.296927</td>\n",
              "      <td>-0.459262</td>\n",
              "      <td>-0.296217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>03</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1346025 rows Ã— 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6ac30b4-2dee-4463-b983-d2b599b21d70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e6ac30b4-2dee-4463-b983-d2b599b21d70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e6ac30b4-2dee-4463-b983-d2b599b21d70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-539086d3-3c81-4f96-b209-23a4d7e6a78d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-539086d3-3c81-4f96-b209-23a4d7e6a78d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-539086d3-3c81-4f96-b209-23a4d7e6a78d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "combined_df"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "from torchmetrics import ConfusionMatrix, F1Score, Precision, Recall, Accuracy, ROC, AUROC\n",
        "from sklearn.metrics import classification_report\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from clearml import Logger as ClearMLLogger  # Add this import\n",
        "\n",
        "from pytorch_lightning.loggers import Logger\n",
        "from pytorch_lightning.utilities import rank_zero_only\n"
      ],
      "metadata": {
        "id": "NDxWxSTDTQFH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClearMLLogger(Logger):\n",
        "    def __init__(self, task_name: str, project_name: str = \"SleepStaging\"):\n",
        "        super().__init__()\n",
        "        self.task = Task.init(project_name=project_name, task_name=task_name)\n",
        "        self.logger = self.task.get_logger()\n",
        "        self.hparams = {}\n",
        "        self.metrics = []\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"ClearML\"\n",
        "\n",
        "    @property\n",
        "    def version(self):\n",
        "        return self.task.id\n",
        "\n",
        "    @rank_zero_only\n",
        "    def log_hyperparams(self, params):\n",
        "        self.hparams.update(params)\n",
        "        self.task.connect(params)\n",
        "\n",
        "    @rank_zero_only\n",
        "    def log_metrics(self, metrics, step):\n",
        "        self.metrics.append(metrics)\n",
        "        for key, value in metrics.items():\n",
        "            self.logger.report_scalar(title=key, series=key, value=value, iteration=step)\n",
        "\n",
        "    @rank_zero_only\n",
        "    def finalize(self, status: str):\n",
        "        self.task.close()\n",
        "\n",
        "    def log_graph(self, model, input_array=None):\n",
        "        pass  # Optional: Implement model graph logging if needed"
      ],
      "metadata": {
        "id": "5VTgdIqJTH6x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 5. Model Architecture (Device-Compatible)\n",
        "# ---------------------------\n",
        "\n",
        "class SleepClassifier(nn.Module):\n",
        "    def __init__(self, input_channels: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 64, 3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "class LightningWrapper(pl.LightningModule):\n",
        "    def __init__(self, model: nn.Module, lr: float, class_labels: List[str], class_weights: Dict[str, float]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.class_labels = class_labels\n",
        "        self.class_weights = torch.tensor(\n",
        "            [class_weights[lbl] for lbl in class_labels],\n",
        "            device=self.device\n",
        "        ).float()\n",
        "\n",
        "        # Initialize metrics on same device as model\n",
        "        self.num_classes = len(class_labels)\n",
        "        self.train_metrics = torchmetrics.MetricCollection({\n",
        "            'acc': Accuracy(task='multiclass', num_classes=self.num_classes),\n",
        "            'f1': F1Score(task='multiclass', num_classes=self.num_classes, average='macro')\n",
        "        })\n",
        "\n",
        "        self.val_metrics = torchmetrics.MetricCollection({\n",
        "            'acc': Accuracy(task='multiclass', num_classes=self.num_classes),\n",
        "            'f1': F1Score(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "            'precision': Precision(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "            'recall': Recall(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "            'auc': AUROC(task='multiclass', num_classes=self.num_classes)\n",
        "        })\n",
        "\n",
        "        # Focal loss for class imbalance\n",
        "        self.loss_fn = FocalLoss(\n",
        "            weight=self.class_weights,\n",
        "            gamma=2.0,\n",
        "            reduction='mean'\n",
        "        )\n",
        "\n",
        "        self.conf_matrix = ConfusionMatrix(task='multiclass', num_classes=self.num_classes)\n",
        "        self.val_outputs = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat, y)\n",
        "\n",
        "        # Update training metrics\n",
        "        self.train_metrics(y_hat.softmax(dim=1), y)\n",
        "        self.log_dict({f'train_{k}': v for k, v in self.train_metrics.items()},\n",
        "                     on_step=False, on_epoch=True)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat, y)\n",
        "\n",
        "        # Get predictions and probabilities\n",
        "        preds = y_hat.argmax(dim=1)\n",
        "        probs = y_hat.softmax(dim=1)\n",
        "\n",
        "        # Update metrics on same device\n",
        "        self.val_metrics(probs, y)\n",
        "        self.conf_matrix(preds, y)\n",
        "\n",
        "        # Store outputs for epoch-end\n",
        "        self.val_outputs.append({\n",
        "            'preds': preds,\n",
        "            'targets': y\n",
        "        })\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # Compute metrics\n",
        "        val_metrics = self.val_metrics.compute()\n",
        "        self.log_dict({f'val_{k}': v for k, v in val_metrics.items()}, prog_bar=True)\n",
        "\n",
        "        # Generate classification report\n",
        "        all_preds = torch.cat([x['preds'].cpu() for x in self.val_outputs]).numpy()\n",
        "        all_targets = torch.cat([x['targets'].cpu() for x in self.val_outputs]).numpy()\n",
        "\n",
        "        report = classification_report(\n",
        "            all_targets, all_preds,\n",
        "            target_names=self.class_labels,\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "        report_df = pd.DataFrame(report).transpose().reset_index()\n",
        "\n",
        "        for logger in self.loggers:\n",
        "            if isinstance(logger, ClearMLLogger):\n",
        "                logger.logger.report_table(\n",
        "                    title='Classification Report',\n",
        "                    series=f'Epoch {self.current_epoch}',\n",
        "                    table_plot=report_df\n",
        "                )\n",
        "\n",
        "        # Reset metrics\n",
        "        self.val_metrics.reset()\n",
        "        self.conf_matrix.reset()\n",
        "        self.val_outputs.clear()\n",
        "\n",
        "    def plot_confusion_matrix(self, cm):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=self.class_labels,\n",
        "                    yticklabels=self.class_labels)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.tight_layout()\n",
        "        return plt.gcf()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='max',\n",
        "            patience=30,\n",
        "            verbose=True\n",
        "        )\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': scheduler,\n",
        "                'monitor': 'val_acc',\n",
        "                'interval': 'epoch',\n",
        "                'frequency': 1\n",
        "            }\n",
        "        }\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.weight)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Training Pipeline (Device-Aware)\n",
        "# ---------------------------\n",
        "def run_training(config: dict):\n",
        "    \"\"\"GPU-compatible training workflow\"\"\"\n",
        "    # Load data with class weights\n",
        "    _, processed_data, class_labels, class_weights = load_and_preprocess_data(config['data_dir'])\n",
        "\n",
        "    # Initialize logger\n",
        "    logger = ClearMLLogger(task_name=\"Final-1DCNN\")\n",
        "    logger.log_hyperparams(config)\n",
        "\n",
        "    try:\n",
        "        # Initialize data module\n",
        "        datamodule = SleepDataModule(processed_data, config)\n",
        "        datamodule.setup()\n",
        "\n",
        "        for test_subject in datamodule.datasets.keys():\n",
        "            print(f\"\\n{'='*40}\\nFold: {test_subject}\")\n",
        "\n",
        "            # Model setup with class weights\n",
        "            model = LightningWrapper.load_from_checkpoint(\n",
        "                ckpt_path,\n",
        "                model=SleepClassifier(19, len(class_labels)),\n",
        "                lr=config['learning_rate'],\n",
        "                class_labels=class_labels,\n",
        "                class_weights=class_weights  # Now passing required parameter\n",
        "                )\n",
        "\n",
        "            # Configure trainer\n",
        "            trainer = Trainer(\n",
        "                accelerator='auto',\n",
        "                devices='auto',\n",
        "                max_epochs=config['max_epochs'],\n",
        "                logger=logger,\n",
        "                callbacks=[\n",
        "                    EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\"),\n",
        "                    ModelCheckpoint(\n",
        "                        dirpath=config['checkpoint_path'],\n",
        "                        filename=f\"best-{test_subject}\",\n",
        "                        monitor=\"val_acc\",\n",
        "                        mode=\"max\"\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Get data loaders\n",
        "            train_loader, val_loader = datamodule.get_loso_splits(test_subject)\n",
        "\n",
        "            # Train/validate\n",
        "            trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.task.get_logger().report_text(f\"Training failed: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        logger.finalize(\"success\")\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Main Execution\n",
        "# # ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Execute analysis with proper parameters\n",
        "    try:\n",
        "        final_results = analyze_results(CONFIG)\n",
        "        print(\"Analysis completed successfully!\")\n",
        "        print(f\"Overall Accuracy: {final_results['overall']['accuracy']:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Analysis failed: {str(e)}\")\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Clean checkpoint directory\n",
        "#     if os.path.exists(CONFIG['checkpoint_path']):\n",
        "#         shutil.rmtree(CONFIG['checkpoint_path'])\n",
        "#     os.makedirs(CONFIG['checkpoint_path'], exist_ok=True)\n",
        "\n",
        "#     run_training(CONFIG)"
      ],
      "metadata": {
        "id": "GVp6yzomLO78",
        "outputId": "23dcfb1e-45bc-4376-ba43-3844da4680b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis failed: name 'analyze_results' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_weights(labels: np.ndarray) -> torch.Tensor:\n",
        "    class_counts = np.bincount(labels)\n",
        "    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "    sample_weights = class_weights[labels]\n",
        "    return sample_weights"
      ],
      "metadata": {
        "id": "E8lQczn3r5jX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 8. Results Analysis\n",
        "# ---------------------------\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def analyze_results(config: dict):\n",
        "    \"\"\"Robust analysis with proper class weights handling\"\"\"\n",
        "    # Load data with class weights\n",
        "    try:\n",
        "        _, processed_data, class_labels, class_weights = load_and_preprocess_data(config['data_dir'])\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Data loading failed: {str(e)}\") from e\n",
        "\n",
        "    if not processed_data:\n",
        "        raise ValueError(\"No processed data available for analysis\")\n",
        "\n",
        "    # Initialize data module\n",
        "    datamodule = SleepDataModule(processed_data, config)\n",
        "    datamodule.setup()\n",
        "\n",
        "    all_true = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    volunteer_results = {}\n",
        "    n_classes = len(class_labels)\n",
        "    label_indices = np.arange(n_classes)\n",
        "\n",
        "# Prediction collection\n",
        "    for vol_id in datamodule.datasets.keys():\n",
        "        ckpt_path = os.path.join(config['checkpoint_path'], f'best-{vol_id}.ckpt')\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            print(f\"Skipping {vol_id} - checkpoint not found\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Add class_weights parameter when loading model\n",
        "            model = LightningWrapper.load_from_checkpoint(\n",
        "                ckpt_path,\n",
        "                model=SleepClassifier(19, len(class_labels)),\n",
        "                lr=config['learning_rate'],\n",
        "                class_labels=class_labels,\n",
        "                class_weights=class_weights  # Add this line\n",
        "            )\n",
        "            model.eval()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {vol_id} model: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # Get test data\n",
        "        try:\n",
        "            _, test_loader = datamodule.get_loso_splits(vol_id)\n",
        "        except KeyError:\n",
        "            print(f\"Skipping {vol_id} - no test data\")\n",
        "            continue\n",
        "\n",
        "        # Run inference\n",
        "        true_labels = []\n",
        "        predictions = []\n",
        "        probabilities = []\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for x, y in test_loader:\n",
        "                    x, y = x.to(model.device), y.to(model.device)\n",
        "                    outputs = model(x)\n",
        "                    probs = torch.softmax(outputs, dim=1)\n",
        "                    preds = torch.argmax(probs, dim=1)  # Keep as tensor\n",
        "\n",
        "                    # Convert to numpy after tensor operations\n",
        "                    true_labels.extend(y.cpu().numpy())\n",
        "                    predictions.extend(preds.cpu().numpy())\n",
        "                    probabilities.extend(probs.cpu().numpy())\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {vol_id} data: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # Validate collected data\n",
        "        if len(true_labels) == 0:\n",
        "            print(f\"Skipping {vol_id} - no test samples\")\n",
        "            continue\n",
        "\n",
        "        # Store results\n",
        "        all_true.extend(true_labels)\n",
        "        all_preds.extend(predictions)\n",
        "        all_probs.extend(probabilities)\n",
        "\n",
        "        # Calculate metrics\n",
        "        try:\n",
        "            auc_score = roc_auc_score(\n",
        "                true_labels,\n",
        "                np.array(probabilities)[:, 1] if n_classes == 2 else probabilities,\n",
        "                multi_class='ovr' if n_classes > 2 else 'raise'\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"AUC calculation failed for {vol_id}: {str(e)}\")\n",
        "            auc_score = np.nan\n",
        "\n",
        "        volunteer_results[vol_id] = {\n",
        "            'samples': len(true_labels),\n",
        "            'accuracy': accuracy_score(true_labels, predictions),\n",
        "            'f1_macro': f1_score(true_labels, predictions, average='macro'),\n",
        "            'auc': auc_score\n",
        "        }\n",
        "\n",
        "    # Final validation\n",
        "    if len(all_true) == 0:\n",
        "        raise ValueError(\"No validation samples collected. Check window_size and data splits.\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_true = np.array(all_true)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # 1. Overall Metrics\n",
        "    print(\"\\n=== Overall Classification Report ===\")\n",
        "    print(classification_report(\n",
        "        all_true,\n",
        "        all_preds,\n",
        "        target_names=class_labels,\n",
        "        labels=label_indices,\n",
        "        zero_division=0,\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    # 2. Detailed Metrics\n",
        "    print(\"\\n=== Detailed Performance ===\")\n",
        "    print(f\"Total Samples: {len(all_true)}\")\n",
        "    print(f\"Accuracy: {accuracy_score(all_true, all_preds):.4f}\")\n",
        "    print(f\"Macro F1: {f1_score(all_true, all_preds, average='macro'):.4f}\")\n",
        "\n",
        "    try:\n",
        "        auc_macro = roc_auc_score(\n",
        "            all_true,\n",
        "            all_probs[:, 1] if n_classes == 2 else all_probs,\n",
        "            multi_class='ovr' if n_classes > 2 else 'raise',\n",
        "            average='macro'\n",
        "        )\n",
        "        print(f\"Macro AUC: {auc_macro:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"AUC calculation failed: {str(e)}\")\n",
        "\n",
        "    # 3. Confusion Matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(all_true, all_preds, labels=label_indices)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_labels,\n",
        "                yticklabels=class_labels)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"final_confusion_matrix.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Save Results\n",
        "    results = {\n",
        "        'overall': {\n",
        "            'accuracy': accuracy_score(all_true, all_preds),\n",
        "            'f1_macro': f1_score(all_true, all_preds, average='macro'),\n",
        "            'auc_macro': auc_macro if 'auc_macro' in locals() else np.nan\n",
        "        },\n",
        "        'per_volunteer': volunteer_results,\n",
        "        'confusion_matrix': cm.tolist()\n",
        "    }\n",
        "\n",
        "    # Save to files\n",
        "    pd.DataFrame(results['per_volunteer']).T.to_csv(\"per_volunteer_results.csv\")\n",
        "    pd.DataFrame(cm, index=class_labels, columns=class_labels)\\\n",
        "      .to_csv(\"confusion_matrix.csv\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Execute analysis (no need to pass common_labels)\n",
        "try:\n",
        "    final_results = analyze_results(CONFIG)\n",
        "except Exception as e:\n",
        "    print(f\"Analysis failed: {str(e)}\")"
      ],
      "metadata": {
        "id": "TWjUhKrOLO2e",
        "outputId": "f18b0fb2-9114-482a-f774-ad2762185a9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading raw data...\n",
            "Common labels: ['N2', 'R']\n",
            "Skipping Vol_01 - checkpoint not found\n",
            "Skipping Vol_02 - checkpoint not found\n",
            "Skipping Vol_03 - checkpoint not found\n",
            "Analysis failed: No validation samples collected. Check window_size and data splits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SyeV71vbC6Af"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}