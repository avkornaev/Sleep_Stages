{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLvX0HO6j3aiS1fP372SLF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avkornaev/Sleep_Stages/blob/main/SleepStages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Instal Libraries"
      ],
      "metadata": {
        "id": "ItfC-WeXWArx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning clearml"
      ],
      "metadata": {
        "id": "VCAdnjnhvDVR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1. Imports & Environment Setup\n",
        "# ---------------------------\n",
        "import os\n",
        "import shutil  # Add this line\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "from clearml import Task\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt  # Add this\n",
        "import seaborn as sns            # Add this\n",
        "\n",
        "import torchaudio  # For audio-specific augmentations\n",
        "import random\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "NTSqV5Uzvquh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set and Initialize"
      ],
      "metadata": {
        "id": "glM38azgWIW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up reproducibility\n",
        "SEED = 42\n",
        "pl.seed_everything(SEED)\n",
        "torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "id": "pdQhGuy9vqsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here to implement Step 2 of the logging instruction as it is shown below\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=ZP02U03C6V5ER4K9VWRNZT7EWA5ZTV\n",
        "%env CLEARML_API_SECRET_KEY=BtA5GXZufr6QGpaqhX1GSKPTvaCt56OLqaNqUGLNoxx2Ye8Ctwbui0Ln5OXVnzUgH4I"
      ],
      "metadata": {
        "id": "7z_tbUUHwCgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'Sleep_Stages' # dataset with the real-world noise\n",
        "#Clone the GitHub repository\n",
        "repo_url = \"https://github.com/avkornaev/Sleep_Stages\"  # Replace with your repo URL\n",
        "!git clone {repo_url}\n",
        "\n",
        "#Navigate to the data folder\n",
        "repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")  # Extract repo name\n",
        "data_dir = os.path.join(repo_name, \"data\")  # Replace \"data\" with your folder name\n",
        "# os.chdir(data_dir)  # Change working directory to the data folder\n",
        "\n",
        "# Verify the data directory\n",
        "if os.path.exists(data_dir):\n",
        "    print(f\"Data directory found: {data_dir}\")\n",
        "else:\n",
        "    print(f\"Data directory not found: {data_dir}\")"
      ],
      "metadata": {
        "id": "hyfkpXxwwPB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 2. Configuration Constants\n",
        "# ---------------------------\n",
        "CONFIG = {\n",
        "    \"data_dir\": \"Sleep_Stages/data\",\n",
        "    \"checkpoint_path\": \"saved_models/\",\n",
        "    \"batch_size\": 128,\n",
        "    \"window_size\": 600,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"max_epochs\": 200,\n",
        "    \"num_workers\": 2,\n",
        "    'focal_gamma': 2.0,  # Adjust based on imbalance severity\n",
        "    'augment_minority': True,\n",
        "    'batch_sampler': 'weighted'\n",
        "}\n",
        "MINORITY_CLASSES = ['R']"
      ],
      "metadata": {
        "id": "Xh9s5d7evqpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funcitons"
      ],
      "metadata": {
        "id": "Qa84bISQWSI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 3. Data Preprocessing Block (Fixed)\n",
        "# ---------------------------\n",
        "def load_and_preprocess_data(directory: str) -> Tuple[pd.DataFrame, Dict[str, tuple], List[str], Dict[str, float]]:\n",
        "    \"\"\"Robust data loader with class weight calculation\"\"\"\n",
        "    # Define column titles based on the sensor specifications\n",
        "    column_titles = [\n",
        "        '1_LDF', '1_T', '1_A365', '1_A460', '1_ANADH',\n",
        "        '2_LDF', '2_T', '2_A365', '2_A460', '2_ANADH',\n",
        "        '3_LDF', '3_T', '3_A365', '3_A460',\n",
        "        '4_LDF', '4_T', '4_A365', '4_A460', '4_ANADH'\n",
        "    ]\n",
        "\n",
        "    # Define validation constraints for each feature type\n",
        "    constraints = {\n",
        "        'LDF': {'min': 0, 'max': 100},\n",
        "        'T': {'min': 25, 'max': 45},\n",
        "        'A365': {'min': 0, 'max': 300},\n",
        "        'A460': {'min': 0, 'max': 300},\n",
        "        'ANADH': {'min': 0, 'max': None}  # Only positive values required\n",
        "    }\n",
        "\n",
        "    # First pass: Load raw data and calculate global statistics\n",
        "    raw_data = defaultdict(list)\n",
        "    all_features = []\n",
        "    label_sets = defaultdict(set)\n",
        "\n",
        "    print(\"Loading raw data...\")\n",
        "    for filename in os.listdir(directory):\n",
        "        if not filename.startswith(\"Vol_\") or not filename.endswith(\".csv.gz\"):\n",
        "            continue\n",
        "\n",
        "        # Extract base volunteer ID (handle split files like Vol_03_1.csv.gz)\n",
        "        base_id = filename.split(\"_\")[1].split(\".\")[0]\n",
        "        if '_' in base_id:  # Handle split files\n",
        "            base_id = base_id.split(\"_\")[0]\n",
        "\n",
        "        try:\n",
        "            # Load data with column names\n",
        "            df = pd.read_csv(os.path.join(directory, filename),\n",
        "                           compression='gzip', skiprows=1, header=None,\n",
        "                           dtype={i: np.float32 for i in range(19)})\n",
        "            df.columns = column_titles + [\"label\"]\n",
        "\n",
        "            # Validate and clean each feature column\n",
        "            for col in column_titles:\n",
        "                feature_type = col.split('_')[-1]\n",
        "                constraint = constraints.get(feature_type)\n",
        "\n",
        "                if constraint:\n",
        "                    min_val = constraint['min']\n",
        "                    max_val = constraint['max']\n",
        "\n",
        "                    # Set invalid values to NaN\n",
        "                    if min_val is not None:\n",
        "                        df.loc[df[col] < min_val, col] = np.nan\n",
        "                    if max_val is not None:\n",
        "                        df.loc[df[col] > max_val, col] = np.nan\n",
        "\n",
        "            # Clean labels\n",
        "            df['label'] = df['label'].str.strip().replace('', np.nan)\n",
        "            df = df.dropna(subset=['label'])\n",
        "\n",
        "            raw_data[base_id].append(df)\n",
        "            label_sets[base_id].update(df['label'].unique())\n",
        "            all_features.append(df[column_titles].values)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Calculate global imputation values\n",
        "    global_mean = np.nanmean(np.concatenate(all_features), axis=0)\n",
        "    global_std = np.nanstd(np.concatenate(all_features), axis=0)\n",
        "\n",
        "    # Determine common labels across all volunteers\n",
        "    common_labels = set(label_sets[next(iter(label_sets))])\n",
        "    for labels in label_sets.values():\n",
        "        common_labels.intersection_update(labels)\n",
        "    common_labels = sorted(common_labels)\n",
        "    if not common_labels:\n",
        "        raise ValueError(\"No common labels found across all volunteers\")\n",
        "    print(f\"Common labels: {common_labels}\")\n",
        "\n",
        "    # Second pass: Process each volunteer with validation\n",
        "    processed_data = {}\n",
        "    combined_dfs = []\n",
        "    label_to_id = {lbl: idx for idx, lbl in enumerate(common_labels)}\n",
        "\n",
        "    for vol_id, dfs in raw_data.items():\n",
        "        full_df = pd.concat(dfs)\n",
        "\n",
        "        # Data cleaning\n",
        "        full_df = full_df[full_df['label'].isin(common_labels)]\n",
        "        for i, col in enumerate(column_titles):\n",
        "            full_df[col] = full_df[col].fillna(global_mean[i])\n",
        "\n",
        "        # Validate label presence\n",
        "        present_labels = full_df['label'].unique()\n",
        "        missing_labels = set(common_labels) - set(present_labels)\n",
        "        if missing_labels:\n",
        "            print(f\"Skipping Vol_{vol_id} - missing labels: {missing_labels}\")\n",
        "            continue\n",
        "\n",
        "        # Normalization\n",
        "        features = (full_df[column_titles].values - global_mean) / (global_std + 1e-8)\n",
        "        labels = full_df['label'].map(label_to_id).values\n",
        "\n",
        "        # Store processed data\n",
        "        processed_data[f\"Vol_{vol_id}\"] = (features.astype(np.float32), labels.astype(np.long))\n",
        "\n",
        "        # Create visualization dataframe\n",
        "        vis_df = pd.DataFrame(features, columns=column_titles)\n",
        "        vis_df['volunteer'] = vol_id\n",
        "        vis_df['label'] = full_df['label'].values\n",
        "        combined_dfs.append(vis_df)\n",
        "\n",
        "    if not processed_data:\n",
        "        raise ValueError(\"No valid volunteers after preprocessing\")\n",
        "\n",
        "    combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
        "\n",
        "    # Calculate class weights\n",
        "    label_counts = combined_df['label'].value_counts()\n",
        "    class_weights = (1. / (label_counts / label_counts.sum())).to_dict()\n",
        "\n",
        "    return combined_df, processed_data, common_labels, class_weights\n",
        "\n",
        "# Update the main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    try:\n",
        "        combined_df, processed_data, common_labels, class_weights = load_and_preprocess_data(CONFIG['data_dir'])\n",
        "\n",
        "        # Visualize dataset statistics\n",
        "        visualize_data(combined_df, common_labels)\n",
        "\n",
        "        # Initialize data module\n",
        "        datamodule = SleepDataModule(processed_data, CONFIG)\n",
        "        datamodule.setup()\n",
        "\n",
        "        # Example LOSO usage\n",
        "        test_subject = list(processed_data.keys())[0]\n",
        "        train_loader, val_loader = datamodule.get_loso_splits(test_subject)\n",
        "        print(f\"\\nLOSO Example: Training on {len(train_loader.dataset)} samples, \"\n",
        "              f\"Validating on {len(val_loader.dataset)} samples\")\n",
        "        print(\"Analysis completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Analysis failed: {str(e)}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Data Visualization (Fixed)\n",
        "# ---------------------------\n",
        "def visualize_data(combined_df: pd.DataFrame, common_labels: List[str]):\n",
        "    \"\"\"Generate comprehensive visualizations of the dataset\"\"\"\n",
        "    # Get feature columns (all columns except 'volunteer' and 'label')\n",
        "    feature_columns = [col for col in combined_df.columns if col not in ['volunteer', 'label']]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.countplot(data=combined_df, x='volunteer', hue='label',\n",
        "                order=sorted(combined_df['volunteer'].unique()),\n",
        "                hue_order=common_labels)\n",
        "    plt.title('Label Distribution by Volunteer')\n",
        "    plt.xlabel('Volunteer ID')\n",
        "    plt.ylabel('Sample Count')\n",
        "    plt.legend(title='Sleep Stage')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance analysis\n",
        "    from sklearn.feature_selection import f_classif\n",
        "    X = combined_df[feature_columns]  # Use actual feature columns\n",
        "    y = combined_df['label'].factorize()[0]\n",
        "\n",
        "    f_values, _ = f_classif(X, y)\n",
        "    feat_importance = pd.DataFrame({\n",
        "        'Feature': feature_columns,  # Use actual feature names\n",
        "        'F-value': f_values\n",
        "    }).sort_values('F-value', ascending=False).head(10)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='F-value', y='Feature', data=feat_importance, palette='viridis')\n",
        "    plt.title('Top 10 Discriminative Features (ANOVA F-value)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Box plots for top 3 features\n",
        "    top_features = feat_importance.head(3)['Feature'].tolist()\n",
        "    for feat in top_features:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x='label', y=feat, data=combined_df,\n",
        "                  order=common_labels)\n",
        "        plt.title(f'Distribution of {feat} by Sleep Stage')\n",
        "        plt.xlabel('Sleep Stage')\n",
        "        plt.ylabel('Normalized Value')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Display sample data\n",
        "    print(\"\\nPreprocessed Data Sample:\")\n",
        "    display(combined_df.sample(5, random_state=SEED))\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Dataset & DataLoaders\n",
        "# ---------------------------\n",
        "class SleepDataset(Dataset):\n",
        "    def __init__(self, features: np.ndarray, labels: np.ndarray, window_size: int):\n",
        "        self.windows, self.labels = self._create_windows(features, labels, window_size)\n",
        "\n",
        "        # Initialize augmentation with random rate selection\n",
        "        self.augment = torch.nn.Sequential(\n",
        "            torchaudio.transforms.TimeStretch(\n",
        "                fixed_rate=random.choice([0.9, 1.0, 1.1])\n",
        "            ),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=15)\n",
        "        )\n",
        "\n",
        "    def _create_windows(self, features, labels, window_size):\n",
        "        windows = []\n",
        "        window_labels = []\n",
        "        for start in range(0, len(features) - window_size + 1, window_size//2):\n",
        "            window = features[start:start+window_size].T\n",
        "            label = labels[start+window_size-1]\n",
        "            windows.append(window)\n",
        "            window_labels.append(label)\n",
        "\n",
        "        # Optimized conversion\n",
        "        return (\n",
        "            torch.FloatTensor(np.stack(windows)),\n",
        "            torch.LongTensor(np.array(window_labels))\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        window = self.windows[idx]\n",
        "        if self.labels[idx] in MINORITY_CLASSES:  # Apply to minority classes\n",
        "            window = self.augment(window)\n",
        "        return window, self.labels[idx]\n",
        "\n",
        "class SleepDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, processed_data: Dict[str, tuple], config: dict):\n",
        "        super().__init__()\n",
        "        self.data = processed_data\n",
        "        self.config = config\n",
        "        self.datasets = {}\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        for vol_id, (feats, lbls) in self.data.items():\n",
        "            self.datasets[vol_id] = SleepDataset(feats, lbls, self.config['window_size'])\n",
        "\n",
        "    def get_loso_splits(self, test_vol: str):\n",
        "        train_feats, train_lbls = [], []\n",
        "        for vol_id, dataset in self.datasets.items():\n",
        "            if vol_id != test_vol:\n",
        "                train_feats.append(dataset.windows)\n",
        "                train_lbls.append(dataset.labels)\n",
        "\n",
        "        train_ds = TensorDataset(torch.cat(train_feats), torch.cat(train_lbls))\n",
        "        test_ds = self.datasets[test_vol]\n",
        "\n",
        "        return (\n",
        "            DataLoader(train_ds, batch_size=self.config['batch_size'],\n",
        "                      shuffle=True, num_workers=self.config['num_workers']),\n",
        "            DataLoader(test_ds, batch_size=self.config['batch_size'],\n",
        "                     num_workers=self.config['num_workers'])\n",
        "        )\n",
        "\n",
        "# ---------------------------\n",
        "# Execution Flow\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    combined_df, processed_data, common_labels, class_weights = load_and_preprocess_data(CONFIG['data_dir'])\n",
        "\n",
        "    # Visualize dataset statistics\n",
        "    visualize_data(combined_df, common_labels)\n",
        "\n",
        "    # Initialize data module\n",
        "    datamodule = SleepDataModule(processed_data, CONFIG)\n",
        "    datamodule.setup()\n",
        "\n",
        "    # Example LOSO usage\n",
        "    test_subject = list(processed_data.keys())[0]\n",
        "    train_loader, val_loader = datamodule.get_loso_splits(test_subject)\n",
        "    print(f\"\\nLOSO Example: Training on {len(train_loader.dataset)} samples, \"\n",
        "          f\"Validating on {len(val_loader.dataset)} samples\")"
      ],
      "metadata": {
        "id": "i5rbezbgLO-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "from torchmetrics import ConfusionMatrix, F1Score, Precision, Recall, Accuracy, ROC, AUROC\n",
        "from sklearn.metrics import classification_report\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from clearml import Logger as ClearMLLogger  # Add this import\n",
        "\n",
        "from pytorch_lightning.loggers import Logger\n",
        "from pytorch_lightning.utilities import rank_zero_only\n"
      ],
      "metadata": {
        "id": "NDxWxSTDTQFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClearMLLogger(Logger):\n",
        "    def __init__(self, task_name: str, project_name: str = \"SleepStaging\"):\n",
        "        super().__init__()\n",
        "        self.task = Task.init(project_name=project_name, task_name=task_name)\n",
        "        self.logger = self.task.get_logger()\n",
        "        self.hparams = {}\n",
        "        self.metrics = []\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"ClearML\"\n",
        "\n",
        "    @property\n",
        "    def version(self):\n",
        "        return self.task.id\n",
        "\n",
        "    @rank_zero_only\n",
        "    def log_hyperparams(self, params):\n",
        "        self.hparams.update(params)\n",
        "        self.task.connect(params)\n",
        "\n",
        "    @rank_zero_only\n",
        "    def log_metrics(self, metrics, step):\n",
        "        self.metrics.append(metrics)\n",
        "        for key, value in metrics.items():\n",
        "            self.logger.report_scalar(title=key, series=key, value=value, iteration=step)\n",
        "\n",
        "    @rank_zero_only\n",
        "    def finalize(self, status: str):\n",
        "        self.task.close()\n",
        "\n",
        "    def log_graph(self, model, input_array=None):\n",
        "        pass  # Optional: Implement model graph logging if needed"
      ],
      "metadata": {
        "id": "5VTgdIqJTH6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 5. Model Architecture (Device-Compatible)\n",
        "# ---------------------------\n",
        "\n",
        "class SleepClassifier(nn.Module):\n",
        "    def __init__(self, input_channels: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 64, 3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "class LightningWrapper(pl.LightningModule):\n",
        "    def __init__(self, model: nn.Module, lr: float, class_labels: List[str], class_weights: Dict[str, float]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.class_labels = class_labels\n",
        "\n",
        "        # Register class weights as buffer for automatic device placement\n",
        "        weights = []\n",
        "        for lbl in class_labels:\n",
        "            if lbl not in class_weights:\n",
        "                raise ValueError(f\"Class label {lbl} missing in class_weights\")\n",
        "            weights.append(class_weights[lbl])\n",
        "        self.register_buffer('class_weights', torch.tensor(weights, dtype=torch.float32))\n",
        "\n",
        "        # Initialize metrics\n",
        "        self.num_classes = len(class_labels)\n",
        "        self.train_metrics = torchmetrics.MetricCollection({\n",
        "            'acc': Accuracy(task='multiclass', num_classes=self.num_classes),\n",
        "            'f1': F1Score(task='multiclass', num_classes=self.num_classes, average='macro')\n",
        "        })\n",
        "\n",
        "        self.val_metrics = torchmetrics.MetricCollection({\n",
        "            'acc': Accuracy(task='multiclass', num_classes=self.num_classes),\n",
        "            'f1': F1Score(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "            'precision': Precision(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "            'recall': Recall(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "            'auc': AUROC(task='multiclass', num_classes=self.num_classes)\n",
        "        })\n",
        "\n",
        "        # Focal loss for class imbalance\n",
        "        self.loss_fn = FocalLoss(\n",
        "            weight=self.class_weights,\n",
        "            gamma=2.0,\n",
        "            reduction='mean'\n",
        "        )\n",
        "\n",
        "        self.conf_matrix = ConfusionMatrix(task='multiclass', num_classes=self.num_classes)\n",
        "        self.val_outputs = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat, y)\n",
        "\n",
        "        # Update training metrics\n",
        "        self.train_metrics(y_hat, y)  # Pass logits directly to metrics\n",
        "        self.log_dict({f'train_{k}': v for k, v in self.train_metrics.items()},\n",
        "                     on_step=False, on_epoch=True)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat, y)\n",
        "\n",
        "        # Get predictions and probabilities\n",
        "        preds = y_hat.argmax(dim=1)\n",
        "        probs = y_hat.softmax(dim=1)\n",
        "\n",
        "        # Update metrics\n",
        "        self.val_metrics(y_hat, y)  # Pass logits directly to metrics\n",
        "        self.conf_matrix(preds, y)\n",
        "\n",
        "        # Store outputs for epoch-end\n",
        "        self.val_outputs.append({\n",
        "            'preds': preds,\n",
        "            'targets': y\n",
        "        })\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # Compute metrics\n",
        "        val_metrics = self.val_metrics.compute()\n",
        "        self.log_dict({f'val_{k}': v for k, v in val_metrics.items()}, prog_bar=True)\n",
        "\n",
        "        # Generate classification report\n",
        "        all_preds = torch.cat([x['preds'].cpu() for x in self.val_outputs]).numpy()\n",
        "        all_targets = torch.cat([x['targets'].cpu() for x in self.val_outputs]).numpy()\n",
        "\n",
        "        report = classification_report(\n",
        "            all_targets, all_preds,\n",
        "            target_names=self.class_labels,\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "        report_df = pd.DataFrame(report).transpose().reset_index()\n",
        "\n",
        "        for logger in self.loggers:\n",
        "            if isinstance(logger, ClearMLLogger):\n",
        "                logger.logger.report_table(\n",
        "                    title='Classification Report',\n",
        "                    series=f'Epoch {self.current_epoch}',\n",
        "                    table_plot=report_df\n",
        "                )\n",
        "\n",
        "        # Reset metrics\n",
        "        self.val_metrics.reset()\n",
        "        self.conf_matrix.reset()\n",
        "        self.val_outputs.clear()\n",
        "\n",
        "    def plot_confusion_matrix(self, cm):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=self.class_labels,\n",
        "                    yticklabels=self.class_labels)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.tight_layout()\n",
        "        return plt.gcf()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='max',\n",
        "            patience=30,\n",
        "            verbose=True\n",
        "        )\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': scheduler,\n",
        "                'monitor': 'val_acc',\n",
        "                'interval': 'epoch',\n",
        "                'frequency': 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "        # Register weight as a buffer if provided\n",
        "        if weight is not None:\n",
        "            self.register_buffer('weight', weight)\n",
        "        else:\n",
        "            self.weight = None\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(\n",
        "            inputs,\n",
        "            targets,\n",
        "            weight=self.weight,  # Uses registered buffer\n",
        "            reduction='none'\n",
        "        )\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Training Pipeline (Device-Aware)\n",
        "# ---------------------------\n",
        "def run_training(config: dict):\n",
        "    \"\"\"GPU-compatible training workflow\"\"\"\n",
        "    # Load data with class weights\n",
        "    _, processed_data, class_labels, class_weights = load_and_preprocess_data(config['data_dir'])\n",
        "\n",
        "    # Initialize logger\n",
        "    logger = ClearMLLogger(task_name=\"Final-1DCNN\")\n",
        "    logger.log_hyperparams(config)\n",
        "\n",
        "    try:\n",
        "        # Initialize data module\n",
        "        datamodule = SleepDataModule(processed_data, config)\n",
        "        datamodule.setup()\n",
        "\n",
        "        for test_subject in datamodule.datasets.keys():\n",
        "            print(f\"\\n{'='*40}\\nFold: {test_subject}\")\n",
        "\n",
        "            # Model setup with class weights\n",
        "            model = LightningWrapper(\n",
        "                model=SleepClassifier(19, len(class_labels)),\n",
        "                lr=config['learning_rate'],\n",
        "                class_labels=class_labels,\n",
        "                class_weights=class_weights\n",
        "            )\n",
        "\n",
        "            # Configure trainer\n",
        "            trainer = Trainer(\n",
        "                accelerator='auto',\n",
        "                devices='auto',\n",
        "                max_epochs=config['max_epochs'],\n",
        "                logger=logger,\n",
        "                callbacks=[\n",
        "                    EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\"),\n",
        "                    ModelCheckpoint(\n",
        "                        dirpath=config['checkpoint_path'],\n",
        "                        filename=f\"best-{test_subject}\",\n",
        "                        monitor=\"val_acc\",\n",
        "                        mode=\"max\"\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Get data loaders\n",
        "            train_loader, val_loader = datamodule.get_loso_splits(test_subject)\n",
        "\n",
        "            # Train/validate\n",
        "            trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.task.get_logger().report_text(f\"Training failed: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        logger.finalize(\"success\")\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Main Execution\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Clean checkpoint directory\n",
        "    if os.path.exists(CONFIG['checkpoint_path']):\n",
        "        shutil.rmtree(CONFIG['checkpoint_path'])\n",
        "    os.makedirs(CONFIG['checkpoint_path'], exist_ok=True)\n",
        "\n",
        "    run_training(CONFIG)\n",
        "\n",
        "    try:\n",
        "        final_results = analyze_results(CONFIG)\n",
        "        print(\"Analysis completed successfully!\")\n",
        "        print(f\"Overall Accuracy: {final_results['overall']['accuracy']:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Analysis failed: {str(e)}\")"
      ],
      "metadata": {
        "id": "opMcKgdaKyLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ---------------------------\n",
        "# # 5. Model Architecture (Device-Compatible)\n",
        "# # ---------------------------\n",
        "\n",
        "# class SleepClassifier(nn.Module):\n",
        "#     def __init__(self, input_channels: int, num_classes: int):\n",
        "#         super().__init__()\n",
        "#         self.feature_extractor = nn.Sequential(\n",
        "#             nn.Conv1d(input_channels, 64, 3, padding=1),\n",
        "#             nn.BatchNorm1d(64),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool1d(2),\n",
        "#             nn.Conv1d(64, 128, 3, padding=1),\n",
        "#             nn.BatchNorm1d(128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool1d(2),\n",
        "#             nn.Conv1d(128, 256, 3, padding=1),\n",
        "#             nn.BatchNorm1d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.AdaptiveAvgPool1d(1)\n",
        "#         )\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(256, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(128, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         features = self.feature_extractor(x)\n",
        "#         return self.classifier(features)\n",
        "\n",
        "# class LightningWrapper(pl.LightningModule):\n",
        "#     def __init__(self, model: nn.Module, lr: float, class_labels: List[str], class_weights: Dict[str, float]):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#         self.lr = lr\n",
        "#         self.class_labels = class_labels\n",
        "#         self.class_weights = torch.tensor(\n",
        "#             [class_weights[lbl] for lbl in class_labels],\n",
        "#             device=self.device\n",
        "#         ).float()\n",
        "\n",
        "#         # Initialize metrics on same device as model\n",
        "#         self.num_classes = len(class_labels)\n",
        "#         self.train_metrics = torchmetrics.MetricCollection({\n",
        "#             'acc': Accuracy(task='multiclass', num_classes=self.num_classes),\n",
        "#             'f1': F1Score(task='multiclass', num_classes=self.num_classes, average='macro')\n",
        "#         })\n",
        "\n",
        "#         self.val_metrics = torchmetrics.MetricCollection({\n",
        "#             'acc': Accuracy(task='multiclass', num_classes=self.num_classes),\n",
        "#             'f1': F1Score(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "#             'precision': Precision(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "#             'recall': Recall(task='multiclass', num_classes=self.num_classes, average='macro'),\n",
        "#             'auc': AUROC(task='multiclass', num_classes=self.num_classes)\n",
        "#         })\n",
        "\n",
        "#         # Focal loss for class imbalance\n",
        "#         self.loss_fn = FocalLoss(\n",
        "#             weight=self.class_weights,\n",
        "#             gamma=2.0,\n",
        "#             reduction='mean'\n",
        "#         )\n",
        "\n",
        "#         self.conf_matrix = ConfusionMatrix(task='multiclass', num_classes=self.num_classes)\n",
        "#         self.val_outputs = []\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         x, y = batch\n",
        "#         y_hat = self(x)\n",
        "#         loss = self.loss_fn(y_hat, y)\n",
        "\n",
        "#         # Update training metrics\n",
        "#         self.train_metrics(y_hat.softmax(dim=1), y)\n",
        "#         self.log_dict({f'train_{k}': v for k, v in self.train_metrics.items()},\n",
        "#                      on_step=False, on_epoch=True)\n",
        "#         self.log('train_loss', loss, prog_bar=True)\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         x, y = batch\n",
        "#         y_hat = self(x)\n",
        "#         loss = self.loss_fn(y_hat, y)\n",
        "\n",
        "#         # Get predictions and probabilities\n",
        "#         preds = y_hat.argmax(dim=1)\n",
        "#         probs = y_hat.softmax(dim=1)\n",
        "\n",
        "#         # Update metrics on same device\n",
        "#         self.val_metrics(probs, y)\n",
        "#         self.conf_matrix(preds, y)\n",
        "\n",
        "#         # Store outputs for epoch-end\n",
        "#         self.val_outputs.append({\n",
        "#             'preds': preds,\n",
        "#             'targets': y\n",
        "#         })\n",
        "\n",
        "#         return loss\n",
        "\n",
        "#     def on_validation_epoch_end(self):\n",
        "#         # Compute metrics\n",
        "#         val_metrics = self.val_metrics.compute()\n",
        "#         self.log_dict({f'val_{k}': v for k, v in val_metrics.items()}, prog_bar=True)\n",
        "\n",
        "#         # Generate classification report\n",
        "#         all_preds = torch.cat([x['preds'].cpu() for x in self.val_outputs]).numpy()\n",
        "#         all_targets = torch.cat([x['targets'].cpu() for x in self.val_outputs]).numpy()\n",
        "\n",
        "#         report = classification_report(\n",
        "#             all_targets, all_preds,\n",
        "#             target_names=self.class_labels,\n",
        "#             output_dict=True,\n",
        "#             zero_division=0\n",
        "#         )\n",
        "#         report_df = pd.DataFrame(report).transpose().reset_index()\n",
        "\n",
        "#         for logger in self.loggers:\n",
        "#             if isinstance(logger, ClearMLLogger):\n",
        "#                 logger.logger.report_table(\n",
        "#                     title='Classification Report',\n",
        "#                     series=f'Epoch {self.current_epoch}',\n",
        "#                     table_plot=report_df\n",
        "#                 )\n",
        "\n",
        "#         # Reset metrics\n",
        "#         self.val_metrics.reset()\n",
        "#         self.conf_matrix.reset()\n",
        "#         self.val_outputs.clear()\n",
        "\n",
        "#     def plot_confusion_matrix(self, cm):\n",
        "#         plt.figure(figsize=(10, 8))\n",
        "#         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "#                     xticklabels=self.class_labels,\n",
        "#                     yticklabels=self.class_labels)\n",
        "#         plt.xlabel('Predicted')\n",
        "#         plt.ylabel('True')\n",
        "#         plt.title('Confusion Matrix')\n",
        "#         plt.tight_layout()\n",
        "#         return plt.gcf()\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "#         scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#             optimizer,\n",
        "#             mode='max',\n",
        "#             patience=30,\n",
        "#             verbose=True\n",
        "#         )\n",
        "#         return {\n",
        "#             'optimizer': optimizer,\n",
        "#             'lr_scheduler': {\n",
        "#                 'scheduler': scheduler,\n",
        "#                 'monitor': 'val_acc',\n",
        "#                 'interval': 'epoch',\n",
        "#                 'frequency': 1\n",
        "#             }\n",
        "#         }\n",
        "# class FocalLoss(nn.Module):\n",
        "#     def __init__(self, weight=None, gamma=2.0, reduction='mean'):\n",
        "#         super().__init__()\n",
        "#         self.weight = weight\n",
        "#         self.gamma = gamma\n",
        "#         self.reduction = reduction\n",
        "\n",
        "#     def forward(self, inputs, targets):\n",
        "#         ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.weight)\n",
        "#         pt = torch.exp(-ce_loss)\n",
        "#         focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "#         if self.reduction == 'mean':\n",
        "#             return focal_loss.mean()\n",
        "#         elif self.reduction == 'sum':\n",
        "#             return focal_loss.sum()\n",
        "#         return focal_loss\n",
        "\n",
        "# # ---------------------------\n",
        "# # 6. Training Pipeline (Device-Aware)\n",
        "# # ---------------------------\n",
        "# def run_training(config: dict):\n",
        "#     \"\"\"GPU-compatible training workflow\"\"\"\n",
        "#     # Load data with class weights\n",
        "#     _, processed_data, class_labels, class_weights = load_and_preprocess_data(config['data_dir'])\n",
        "\n",
        "#     # Initialize logger\n",
        "#     logger = ClearMLLogger(task_name=\"Final-1DCNN\")\n",
        "#     logger.log_hyperparams(config)\n",
        "\n",
        "#     try:\n",
        "#         # Initialize data module\n",
        "#         datamodule = SleepDataModule(processed_data, config)\n",
        "#         datamodule.setup()\n",
        "\n",
        "#         for test_subject in datamodule.datasets.keys():\n",
        "#             print(f\"\\n{'='*40}\\nFold: {test_subject}\")\n",
        "\n",
        "#             # Model setup with class weights\n",
        "#             model = LightningWrapper.load_from_checkpoint(\n",
        "#                 ckpt_path,\n",
        "#                 model=SleepClassifier(19, len(class_labels)),\n",
        "#                 lr=config['learning_rate'],\n",
        "#                 class_labels=class_labels,\n",
        "#                 class_weights=class_weights  # Now passing required parameter\n",
        "#                 )\n",
        "\n",
        "#             # Configure trainer\n",
        "#             trainer = Trainer(\n",
        "#                 accelerator='auto',\n",
        "#                 devices='auto',\n",
        "#                 max_epochs=config['max_epochs'],\n",
        "#                 logger=logger,\n",
        "#                 callbacks=[\n",
        "#                     EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\"),\n",
        "#                     ModelCheckpoint(\n",
        "#                         dirpath=config['checkpoint_path'],\n",
        "#                         filename=f\"best-{test_subject}\",\n",
        "#                         monitor=\"val_acc\",\n",
        "#                         mode=\"max\"\n",
        "#                     )\n",
        "#                 ]\n",
        "#             )\n",
        "\n",
        "#             # Get data loaders\n",
        "#             train_loader, val_loader = datamodule.get_loso_splits(test_subject)\n",
        "\n",
        "#             # Train/validate\n",
        "#             trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.task.get_logger().report_text(f\"Training failed: {str(e)}\")\n",
        "#         raise\n",
        "#     finally:\n",
        "#         logger.finalize(\"success\")\n",
        "\n",
        "# # ---------------------------\n",
        "# # 7. Main Execution\n",
        "# # # ---------------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Execute analysis with proper parameters\n",
        "#     try:\n",
        "#         final_results = analyze_results(CONFIG)\n",
        "#         print(\"Analysis completed successfully!\")\n",
        "#         print(f\"Overall Accuracy: {final_results['overall']['accuracy']:.2f}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Analysis failed: {str(e)}\")\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Clean checkpoint directory\n",
        "#     if os.path.exists(CONFIG['checkpoint_path']):\n",
        "#         shutil.rmtree(CONFIG['checkpoint_path'])\n",
        "#     os.makedirs(CONFIG['checkpoint_path'], exist_ok=True)\n",
        "\n",
        "#     run_training(CONFIG)"
      ],
      "metadata": {
        "id": "GVp6yzomLO78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_weights(labels: np.ndarray) -> torch.Tensor:\n",
        "    class_counts = np.bincount(labels)\n",
        "    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "    sample_weights = class_weights[labels]\n",
        "    return sample_weights"
      ],
      "metadata": {
        "id": "E8lQczn3r5jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 8. Results Analysis\n",
        "# ---------------------------\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def analyze_results(config: dict):\n",
        "    \"\"\"Robust analysis with proper class weights handling\"\"\"\n",
        "    # Load data with class weights\n",
        "    try:\n",
        "        _, processed_data, class_labels, class_weights = load_and_preprocess_data(config['data_dir'])\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Data loading failed: {str(e)}\") from e\n",
        "\n",
        "    if not processed_data:\n",
        "        raise ValueError(\"No processed data available for analysis\")\n",
        "\n",
        "    # Initialize data module\n",
        "    datamodule = SleepDataModule(processed_data, config)\n",
        "    datamodule.setup()\n",
        "\n",
        "    all_true = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    volunteer_results = {}\n",
        "    n_classes = len(class_labels)\n",
        "    label_indices = np.arange(n_classes)\n",
        "\n",
        "# Prediction collection\n",
        "    for vol_id in datamodule.datasets.keys():\n",
        "        ckpt_path = os.path.join(config['checkpoint_path'], f'best-{vol_id}.ckpt')\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            print(f\"Skipping {vol_id} - checkpoint not found\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Add class_weights parameter when loading model\n",
        "            model = LightningWrapper.load_from_checkpoint(\n",
        "                ckpt_path,\n",
        "                model=SleepClassifier(19, len(class_labels)),\n",
        "                lr=config['learning_rate'],\n",
        "                class_labels=class_labels,\n",
        "                class_weights=class_weights  # Add this line\n",
        "            )\n",
        "            model.eval()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {vol_id} model: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # Get test data\n",
        "        try:\n",
        "            _, test_loader = datamodule.get_loso_splits(vol_id)\n",
        "        except KeyError:\n",
        "            print(f\"Skipping {vol_id} - no test data\")\n",
        "            continue\n",
        "\n",
        "        # Run inference\n",
        "        true_labels = []\n",
        "        predictions = []\n",
        "        probabilities = []\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for x, y in test_loader:\n",
        "                    x, y = x.to(model.device), y.to(model.device)\n",
        "                    outputs = model(x)\n",
        "                    probs = torch.softmax(outputs, dim=1)\n",
        "                    preds = torch.argmax(probs, dim=1)  # Keep as tensor\n",
        "\n",
        "                    # Convert to numpy after tensor operations\n",
        "                    true_labels.extend(y.cpu().numpy())\n",
        "                    predictions.extend(preds.cpu().numpy())\n",
        "                    probabilities.extend(probs.cpu().numpy())\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {vol_id} data: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # Validate collected data\n",
        "        if len(true_labels) == 0:\n",
        "            print(f\"Skipping {vol_id} - no test samples\")\n",
        "            continue\n",
        "\n",
        "        # Store results\n",
        "        all_true.extend(true_labels)\n",
        "        all_preds.extend(predictions)\n",
        "        all_probs.extend(probabilities)\n",
        "\n",
        "        # Calculate metrics\n",
        "        try:\n",
        "            auc_score = roc_auc_score(\n",
        "                true_labels,\n",
        "                np.array(probabilities)[:, 1] if n_classes == 2 else probabilities,\n",
        "                multi_class='ovr' if n_classes > 2 else 'raise'\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"AUC calculation failed for {vol_id}: {str(e)}\")\n",
        "            auc_score = np.nan\n",
        "\n",
        "        volunteer_results[vol_id] = {\n",
        "            'samples': len(true_labels),\n",
        "            'accuracy': accuracy_score(true_labels, predictions),\n",
        "            'f1_macro': f1_score(true_labels, predictions, average='macro'),\n",
        "            'auc': auc_score\n",
        "        }\n",
        "\n",
        "    # Final validation\n",
        "    if len(all_true) == 0:\n",
        "        raise ValueError(\"No validation samples collected. Check window_size and data splits.\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_true = np.array(all_true)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # 1. Overall Metrics\n",
        "    print(\"\\n=== Overall Classification Report ===\")\n",
        "    print(classification_report(\n",
        "        all_true,\n",
        "        all_preds,\n",
        "        target_names=class_labels,\n",
        "        labels=label_indices,\n",
        "        zero_division=0,\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    # 2. Detailed Metrics\n",
        "    print(\"\\n=== Detailed Performance ===\")\n",
        "    print(f\"Total Samples: {len(all_true)}\")\n",
        "    print(f\"Accuracy: {accuracy_score(all_true, all_preds):.4f}\")\n",
        "    print(f\"Macro F1: {f1_score(all_true, all_preds, average='macro'):.4f}\")\n",
        "\n",
        "    try:\n",
        "        auc_macro = roc_auc_score(\n",
        "            all_true,\n",
        "            all_probs[:, 1] if n_classes == 2 else all_probs,\n",
        "            multi_class='ovr' if n_classes > 2 else 'raise',\n",
        "            average='macro'\n",
        "        )\n",
        "        print(f\"Macro AUC: {auc_macro:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"AUC calculation failed: {str(e)}\")\n",
        "\n",
        "    # 3. Confusion Matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(all_true, all_preds, labels=label_indices)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_labels,\n",
        "                yticklabels=class_labels)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"final_confusion_matrix.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Save Results\n",
        "    results = {\n",
        "        'overall': {\n",
        "            'accuracy': accuracy_score(all_true, all_preds),\n",
        "            'f1_macro': f1_score(all_true, all_preds, average='macro'),\n",
        "            'auc_macro': auc_macro if 'auc_macro' in locals() else np.nan\n",
        "        },\n",
        "        'per_volunteer': volunteer_results,\n",
        "        'confusion_matrix': cm.tolist()\n",
        "    }\n",
        "\n",
        "    # Save to files\n",
        "    pd.DataFrame(results['per_volunteer']).T.to_csv(\"per_volunteer_results.csv\")\n",
        "    pd.DataFrame(cm, index=class_labels, columns=class_labels)\\\n",
        "      .to_csv(\"confusion_matrix.csv\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Execute analysis (no need to pass common_labels)\n",
        "try:\n",
        "    final_results = analyze_results(CONFIG)\n",
        "except Exception as e:\n",
        "    print(f\"Analysis failed: {str(e)}\")"
      ],
      "metadata": {
        "id": "TWjUhKrOLO2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/Sleep_Stages/data')"
      ],
      "metadata": {
        "id": "SyeV71vbC6Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5W1Ujuf_q9Z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}