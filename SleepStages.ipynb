{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMK/trvizSsX7xtSjQiv5bA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avkornaev/Sleep_Stages/blob/main/SleepStages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sleep Stages\n",
        "*March, 21, 2025.*"
      ],
      "metadata": {
        "id": "dviRg5xn5mza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "fS8QkXU47uNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given data files for a several volunteers. Each file contain time series multi-sensory data and their processing results (19 columns total) and a column for label at each time step. The equipment used for data collection implements non-invasive optical diagnostic methods for recording peripheral blood flow parameters using laser Doppler flowmetry (LDF) and tissue oxidative metabolism through fluorescence spectroscopy (FS). The data were recorded during several hour of sleep. The time step was 0.05 s for all the sensors. The label is a sleep stage (NREM1, NREM2, REM, Wakefulness). Some of the data is missed. [Repo\n",
        "](https://github.com/avkornaev/Sleep_Stages) of the project."
      ],
      "metadata": {
        "id": "EvP6IWSB5s_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks and Requirements  \n",
        "\n",
        "- Review the [Lightning framework](https://lightning.ai/docs/pytorch/stable/) (Level Up, Core API, Optional API sections of the manual).  \n",
        "- Briefly review the [ClearML](https://clear.ml/docs/latest/docs/integrations/pytorch_lightning/) documentation."
      ],
      "metadata": {
        "id": "fZWswAni70vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of simulation models"
      ],
      "metadata": {
        "id": "kN9yU_U6766c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Install Libraries"
      ],
      "metadata": {
        "id": "9mpwlpoX5TI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMPnV0nV4_-I",
        "outputId": "09e69a9b-4677-4bca-d674-3c1edee715d5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: clearml in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.2)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (25.3.0)\n",
            "Requirement already satisfied: furl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.1.4)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.0.2)\n",
            "Requirement already satisfied: pathlib2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.3.7.post1)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (11.1.0)\n",
            "Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.11/dist-packages (from clearml) (5.9.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from clearml) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.32.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.3.0)\n",
            "Requirement already satisfied: pyjwt<2.10.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from clearml) (2.9.0)\n",
            "Requirement already satisfied: referencing<0.40 in /usr/local/lib/python3.11/dist-packages (from clearml) (0.36.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.14)\n",
            "Requirement already satisfied: orderedmultidict>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from furl>=2.0.0->clearml) (1.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6.0->clearml) (0.23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->clearml) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning clearml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pytorch modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
        "from torchvision import datasets, transforms, models\n",
        "#scipy\n",
        "from scipy.stats import mode\n",
        "#sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "\n",
        "#Numpy\n",
        "import numpy as np\n",
        "#Pandas\n",
        "import pandas as pd\n",
        "#Lightning & logging\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "#Data observation\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Logging\n",
        "from clearml import Task"
      ],
      "metadata": {
        "id": "fDHrafErmo43"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the Models"
      ],
      "metadata": {
        "id": "ealb85K93wDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulation Settings"
      ],
      "metadata": {
        "id": "GHIKBWI93-zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the current directory"
      ],
      "metadata": {
        "id": "kiYPAzh54gjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd() #returns the current working directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "phE7U1vu31BR",
        "outputId": "4977672d-8032-4933-d390-2dbed9bd43c0"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/\")\n",
        "print(f'CHECKPOINT_PATH: {CHECKPOINT_PATH}')\n",
        "\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EVwTLDiNYyc",
        "outputId": "c10e992e-c407-40e3-ad9d-57633ab7e507"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKPOINT_PATH: saved_models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the reproducibility options"
      ],
      "metadata": {
        "id": "3WK77wcO6sfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for setting the seed to implement parallel tests\n",
        "SEEDS =  [42] #[42, 0, 17, 9, 3, 16, 2]\n",
        "SEED = 42 # random seed by default\n",
        "pl.seed_everything(SEED)\n",
        "\n",
        "# Determine the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prioritizes speed but may reduce precision\n",
        "torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YX7JeP93-TZ",
        "outputId": "0b2cce17-7558-45a3-8af7-2984a8a79b08"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging"
      ],
      "metadata": {
        "id": "_7ULmzow4jSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To configure ClearML in your Colab environment, follow these steps:\n",
        "\n",
        "---\n",
        "\n",
        "*Step 1: Create a ClearML Account*\n",
        "1. Go to the [ClearML website](https://clear.ml/).\n",
        "2. Sign up for a free account if you donâ€™t already have one.\n",
        "3. Once registered, log in to your ClearML account.\n",
        "\n",
        "---\n",
        "\n",
        "*Step 2: Get Your ClearML Credentials*\n",
        "1. After logging in, navigate to the **Settings** page (click on your profile icon in the top-right corner and select **Settings**).\n",
        "2. Under the **Workspace** section, find your **+ Create new credentials**.\n",
        "3. Copy these credentials for a Jupiter notebook into the code cell below.\n",
        "\n",
        "---\n",
        "\n",
        "*Step 3: Accessing the ClearML Dashboard*\n",
        "1. Go to your ClearML dashboard (https://app.clear.ml).\n",
        "2. Navigate to the **Projects** section to see your experiments.\n",
        "3. Click on the experiment (e.g., `Lab_1`) to view detailed metrics, logs, and artifacts.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "C97DLT0gK37A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter your code here to implement Step 2 of the logging instruction as it is shown below\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=ZP02U03C6V5ER4K9VWRNZT7EWA5ZTV\n",
        "%env CLEARML_API_SECRET_KEY=BtA5GXZufr6QGpaqhX1GSKPTvaCt56OLqaNqUGLNoxx2Ye8Ctwbui0Ln5OXVnzUgH4I"
      ],
      "metadata": {
        "id": "lTXMGNya32_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08c1813-2eaa-412f-bb6f-179251fad365"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=ZP02U03C6V5ER4K9VWRNZT7EWA5ZTV\n",
            "env: CLEARML_API_SECRET_KEY=BtA5GXZufr6QGpaqhX1GSKPTvaCt56OLqaNqUGLNoxx2Ye8Ctwbui0Ln5OXVnzUgH4I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "BujHK4sw7cA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary"
      ],
      "metadata": {
        "id": "Wb0uJtxz-E--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'Sleep_Stages' # dataset with the real-world noise\n",
        "#Clone the GitHub repository\n",
        "repo_url = \"https://github.com/avkornaev/Sleep_Stages\"  # Replace with your repo URL\n",
        "!git clone {repo_url}\n",
        "\n",
        "#Navigate to the data folder\n",
        "repo_name = repo_url.split(\"/\")[-1].replace(\".git\", \"\")  # Extract repo name\n",
        "data_dir = os.path.join(repo_name, \"data\")  # Replace \"data\" with your folder name\n",
        "# os.chdir(data_dir)  # Change working directory to the data folder\n",
        "\n",
        "# Verify the data directory\n",
        "if os.path.exists(data_dir):\n",
        "    print(f\"Data directory found: {data_dir}\")\n",
        "else:\n",
        "    print(f\"Data directory not found: {data_dir}\")"
      ],
      "metadata": {
        "id": "hWRDBJbO7k_u",
        "outputId": "2713e2cc-3318-43db-c1d8-dea48e361db2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Sleep_Stages' already exists and is not an empty directory.\n",
            "Data directory found: Sleep_Stages/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collect parameters"
      ],
      "metadata": {
        "id": "GwaBDKEvD5ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model parameters\n",
        "LOSS_FUN = 'CE' # 'CE','CELoss'(custom), 'N', 'B', etc.\n",
        "ARCHITECTURE = '1DCNN' #\n",
        "\n",
        "#Collect the parameters (hyperparams and others)\n",
        "hparams = {\n",
        "    \"seed\": SEED,\n",
        "    \"lr\": 0.001,\n",
        "    'weight_decay': 0.0,\n",
        "    \"dropout\": 0.0,\n",
        "    \"bs\": 128,\n",
        "    \"num_workers\": 2,\n",
        "    \"num_epochs\": 2,\n",
        "    \"criterion\": LOSS_FUN,\n",
        "    \"architecture\": ARCHITECTURE,\n",
        "    \"window_size\": 600,\n",
        "    'label_smoothing': 0.0,\n",
        "    }\n",
        "\n",
        "#Visualization\n",
        "vis_params = {\n",
        "    'fig_size': 5,\n",
        "    'num_samples': 5,\n",
        "    'num_bins': 50,\n",
        "}"
      ],
      "metadata": {
        "id": "NSyUzYCSD0v3"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "OQsu5FcbFfwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightning"
      ],
      "metadata": {
        "id": "ZAOMnXlqFofC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data module"
      ],
      "metadata": {
        "id": "2N2vslXcUvGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(directory, allowed_labels=None):\n",
        "    \"\"\"\n",
        "    Load and merge volunteer data with NaN handling.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Path to the directory containing the data files.\n",
        "        allowed_labels (list, optional): List of labels to include. If None, only labels\n",
        "                                         present for all volunteers are used.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are volunteer IDs (e.g., \"Vol_01\") and values are\n",
        "              tuples of (features, labels).\n",
        "    \"\"\"\n",
        "    volunteer_data = defaultdict(list)\n",
        "\n",
        "    # Load data for all volunteers\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".csv.gz\") and filename.startswith(\"Vol_\"):\n",
        "            volunteer_id = filename.split(\"_\")[1].split(\".\")[0]\n",
        "            file_path = os.path.join(directory, filename)\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv(file_path, compression='gzip')\n",
        "                # Check for empty/nan-only data\n",
        "                if df.drop(columns=['label']).isna().all().all():\n",
        "                    print(f\"Warning: {filename} contains only NaN values. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Forward fill missing values\n",
        "                df.ffill(inplace=True)\n",
        "                # Drop remaining NaN rows\n",
        "                df.dropna(inplace=True)\n",
        "\n",
        "                volunteer_data[volunteer_id].append(df)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    # Determine common labels if not provided\n",
        "    if allowed_labels is None:\n",
        "        all_labels = set()\n",
        "        for volunteer_id, dfs in volunteer_data.items():\n",
        "            if dfs:\n",
        "                df = pd.concat(dfs, ignore_index=True)\n",
        "                all_labels.update(df['label'].unique())\n",
        "\n",
        "        # Find labels present for all volunteers\n",
        "        common_labels = all_labels.copy()\n",
        "        for volunteer_id, dfs in volunteer_data.items():\n",
        "            if dfs:\n",
        "                df = pd.concat(dfs, ignore_index=True)\n",
        "                common_labels.intersection_update(df['label'].unique())\n",
        "\n",
        "        if not common_labels:\n",
        "            raise ValueError(\"No common labels found across all volunteers.\")\n",
        "\n",
        "        allowed_labels = list(common_labels)\n",
        "        print(f\"Using common labels: {allowed_labels}\")\n",
        "\n",
        "    # Process each volunteer's data\n",
        "    merged_data = {}\n",
        "    for volunteer_id, dfs in volunteer_data.items():\n",
        "        if not dfs:  # Skip volunteers with all files invalid\n",
        "            print(f\"Warning: No valid data for Vol_{volunteer_id}\")\n",
        "            continue\n",
        "\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        # Filter rows with allowed labels\n",
        "        df = df[df['label'].isin(allowed_labels)]\n",
        "\n",
        "        # Final NaN check\n",
        "        if df.empty:\n",
        "            print(f\"Warning: No valid data remaining for Vol_{volunteer_id}\")\n",
        "            continue\n",
        "\n",
        "        # Normalize features (excluding label column)\n",
        "        features = df.iloc[:, :-1].values.astype(np.float32)\n",
        "        features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
        "\n",
        "        # Convert labels to integers\n",
        "        label_to_code = {label: idx for idx, label in enumerate(allowed_labels)}\n",
        "        labels = df['label'].map(label_to_code).values.astype(np.int64)\n",
        "\n",
        "        merged_data[f\"Vol_{volunteer_id}\"] = (features, labels)\n",
        "\n",
        "    return merged_data"
      ],
      "metadata": {
        "id": "l4LT-NyRgAWZ"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SleepStageDataset(Dataset):\n",
        "    def __init__(self, features, labels, window_size=600, stride=300):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features (np.ndarray): Input features of shape (num_samples, num_features).\n",
        "            labels (np.ndarray): Corresponding labels of shape (num_samples,).\n",
        "            window_size (int): Size of the sliding window.\n",
        "            stride (int): Stride for the sliding window.\n",
        "        \"\"\"\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.windows, self.window_labels = self._create_windows()\n",
        "\n",
        "    def _create_windows(self):\n",
        "        \"\"\"\n",
        "        Creates sliding windows from the time-series data.\n",
        "        Returns:\n",
        "            windows (np.ndarray): Windows of shape (num_windows, num_features, window_size).\n",
        "            window_labels (np.ndarray): Labels for each window of shape (num_windows,).\n",
        "        \"\"\"\n",
        "        windows = []\n",
        "        window_labels = []\n",
        "        n_samples = self.features.shape[0]\n",
        "\n",
        "        for start in range(0, n_samples - self.window_size + 1, self.stride):\n",
        "            end = start + self.window_size\n",
        "            window = self.features[start:end]  # Shape: (window_size, num_features)\n",
        "            label = self.labels[end - 1]  # Use the label at the end of the window\n",
        "            windows.append(window.T)  # Transpose to (num_features, window_size)\n",
        "            window_labels.append(label)\n",
        "\n",
        "        return np.array(windows), np.array(window_labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            x (torch.Tensor): Input tensor of shape (num_features, window_size).\n",
        "            y (torch.Tensor): Label tensor (scalar).\n",
        "        \"\"\"\n",
        "        x = torch.tensor(self.windows[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.window_labels[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "class SleepDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, merged_data, batch_size=32, window_size=600):\n",
        "        super().__init__()\n",
        "        self.merged_data = merged_data\n",
        "        self.batch_size = batch_size\n",
        "        self.window_size = window_size\n",
        "        self.subjects = list(merged_data.keys())\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"Prepares all subject datasets.\"\"\"\n",
        "        self.datasets = {}\n",
        "        for vol_id, (features, labels) in self.merged_data.items():\n",
        "            self.datasets[vol_id] = SleepStageDataset(features, labels, self.window_size)\n",
        "\n",
        "    def get_loso_splits(self, test_subject):\n",
        "        \"\"\"\n",
        "        Generates LOSO splits for a given test subject.\n",
        "        Returns:\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "            val_loader (DataLoader): DataLoader for validation data.\n",
        "        \"\"\"\n",
        "        train_subs = [s for s in self.subjects if s != test_subject]\n",
        "\n",
        "        # Combine training subjects\n",
        "        train_data = torch.cat([torch.tensor(self.datasets[s].windows, dtype=torch.float32) for s in train_subs])\n",
        "        train_labels = torch.cat([torch.tensor(self.datasets[s].window_labels, dtype=torch.long) for s in train_subs])\n",
        "\n",
        "        # Test subject\n",
        "        test_data = torch.tensor(self.datasets[test_subject].windows, dtype=torch.float32)\n",
        "        test_labels = torch.tensor(self.datasets[test_subject].window_labels, dtype=torch.long)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            TensorDataset(train_data, train_labels),\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            TensorDataset(test_data, test_labels),\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader"
      ],
      "metadata": {
        "id": "LbTBeiQvPgwt"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training module"
      ],
      "metadata": {
        "id": "JBWSuYApFu7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SleepClassifier(pl.LightningModule):\n",
        "    def __init__(self, input_channels=19, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.AdaptiveAvgPool1d(1),\n",
        "\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(256, num_classes)\n",
        "        )\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat, y)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat, y)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "\n",
        "        # Track predictions and labels\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        return {\"preds\": preds, \"labels\": y}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        # Aggregate predictions and labels\n",
        "        preds = torch.cat([x[\"preds\"] for x in outputs])\n",
        "        labels = torch.cat([x[\"labels\"] for x in outputs])\n",
        "\n",
        "        # Compute accuracy\n",
        "        acc = (preds == labels).float().mean()\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        return {\"val_acc\": acc}"
      ],
      "metadata": {
        "id": "DdwlFavDZeB-"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "SpGtoJicGCJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SleepStageCNN(pl.LightningModule):\n",
        "#     def __init__(self, input_channels=19, num_classes=4):  # Update input_channels\n",
        "#         super().__init__()\n",
        "#         self.model = torch.nn.Sequential(\n",
        "#             torch.nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n",
        "#             torch.nn.BatchNorm1d(64),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.MaxPool1d(2),\n",
        "\n",
        "#             torch.nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "#             torch.nn.BatchNorm1d(128),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.MaxPool1d(2),\n",
        "\n",
        "#             torch.nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "#             torch.nn.BatchNorm1d(256),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.AdaptiveAvgPool1d(1),\n",
        "\n",
        "#             torch.nn.Flatten(),\n",
        "#             torch.nn.Linear(256, num_classes)\n",
        "#         )\n",
        "#         self.loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "oA84H5z_t4oY"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss functions"
      ],
      "metadata": {
        "id": "rHm8wnbnGEnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a loss function class, or use a standart one."
      ],
      "metadata": {
        "id": "8z05aQ7cQTm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cross entropy loss maden from scratch (just in case)\n",
        "# class CELoss(nn.Module):\n",
        "#     def __init__(self, params=hparams):\n",
        "#         super(CELoss, self).__init__()\n",
        "#         self.smoothing = params.get('label_smoothing', 0.1)  # Default smoothing value\n",
        "#         self.num_classes = params.get('n_classes', 10)\n",
        "#         self.inv_smoothing = 1.0 - self.smoothing  # Probability for the correct class\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         \"\"\"\n",
        "#         x: Model output (logits)\n",
        "#             - Shape: (batch_size, num_classes)\n",
        "#         y: Labels\n",
        "#             - Shape: (batch_size,)\n",
        "#         \"\"\"\n",
        "#         # Apply label smoothing to the one-hot encoded labels\n",
        "#         with torch.no_grad():\n",
        "#             yoh = torch.zeros_like(x)  # Create a one-hot encoded version of y\n",
        "#             yoh.fill_(self.smoothing / (self.num_classes - 1))  # Fill with smoothed values\n",
        "#             yoh.scatter_(1, y.unsqueeze(1), self.inv_smoothing)  # Set correct class to 1 - smoothing\n",
        "\n",
        "#         # Compute the cross-entropy loss between logits and smoothed labels\n",
        "#         log_probs = F.log_softmax(x, dim=1)  # Log probabilities\n",
        "#         loss = -(yoh * log_probs).sum(dim=1).mean()  # Sum over classes and mean over batch\n",
        "\n",
        "#         return loss"
      ],
      "metadata": {
        "id": "8auVRUCKGEG2"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class NLoss(nn.Module):\n",
        "#     def __init__(self, params=hparams):\n",
        "#         super(NLoss, self).__init__()\n",
        "#         self.smoothing =   params.get('label_smoothing', 0.0)\n",
        "#         self.num_classes = params.get('n_classes', 10)\n",
        "#         self.inv_smoothing = 1.0 - self.smoothing  # Probability for the correct class\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         \"\"\"\n",
        "#         x: Model output (logits + log variance)\n",
        "#             - x[:, :self.num_classes]: Logits for class probabilities (h)\n",
        "#             - x[:, self.num_classes:]: Logarithmic variance (s)\n",
        "#         y: Labels\n",
        "#         \"\"\"\n",
        "#         # Split the model output into predictions (h) and log variance (s)\n",
        "#         logits = x[:, :self.num_classes]  # Predictions (h)\n",
        "#         log_var = x[:, self.num_classes:]  # Logarithmic variance (s)\n",
        "\n",
        "#         # Apply label smoothing to the one-hot encoded labels\n",
        "#         with torch.no_grad():\n",
        "#             yoh = torch.zeros_like(logits)\n",
        "#             yoh.fill_(self.smoothing / (self.num_classes - 1))\n",
        "#             yoh.scatter_(1, y.data.unsqueeze(1), self.inv_smoothing)\n",
        "\n",
        "#         # Compute the squared differences between predictions and smoothed labels\n",
        "#         squared_diff = torch.pow(yoh - logits, 2)  # (y_k - h_k)^2\n",
        "\n",
        "#         # Compute the exponential of the negative log variance (e^{-s})\n",
        "#         exp_neg_log_var = torch.exp(-log_var)\n",
        "\n",
        "#         # Compute the first term of the loss: e^{-s} * sum((y_k - h_k)^2)\n",
        "#         term1 = exp_neg_log_var * squared_diff.sum(dim=1)\n",
        "\n",
        "#         # Compute the second term of the loss: N * s\n",
        "#         term2 = self.num_classes * log_var\n",
        "\n",
        "#         # Combine the terms and compute the mean over the batch\n",
        "#         loss = (term1 + term2).mean()\n",
        "\n",
        "#         return loss"
      ],
      "metadata": {
        "id": "An6Q9DUNK3SE"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BLoss(nn.Module):\n",
        "    def __init__(self, params=hparams):\n",
        "        super(BLoss, self).__init__()\n",
        "        self.smoothing =   params.get('label_smoothing', 0.0)\n",
        "        self.num_classes = params.get('n_classes', 10)\n",
        "        self.inv_smoothing = 1.0 - self.smoothing  # Probability for the correct class\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Extract certainty and probabilities from the model output\n",
        "        certainty = torch.sigmoid(x[:, self.num_classes:])  # Certainty values\n",
        "        logits = x[:, :self.num_classes]  # Logits for class probabilities\n",
        "        prob = F.softmax(logits, dim=1)  # Softmax probabilities\n",
        "\n",
        "        # Compute cosine similarity between predictions and labels\n",
        "        cos = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "        # Apply label smoothing to the one-hot encoded labels\n",
        "        with torch.no_grad():\n",
        "            yoh = torch.zeros_like(logits)\n",
        "            yoh.fill_(self.smoothing / (self.num_classes - 1))\n",
        "            yoh.scatter_(1, y.data.unsqueeze(1), self.inv_smoothing)\n",
        "\n",
        "\n",
        "        # Compute the terms of the loss\n",
        "        cosyh = cos(yoh, prob)\n",
        "        delta = yoh * prob  # Element-wise product of one-hot labels and probabilities\n",
        "        entropy_term = delta * torch.log(delta + 1e-10)  # Entropy term (avoid log(0))\n",
        "\n",
        "        # Loss terms\n",
        "        loss0 = -cosyh * torch.log(certainty / self.num_classes + 1e-10)  # First term\n",
        "        loss1 = -(self.num_classes - 1) * (1 - cosyh) * torch.log((1 - certainty) / self.num_classes + 1e-10)  # Second term\n",
        "\n",
        "        # Combine the terms and compute the mean over the batch\n",
        "        loss = (entropy_term.sum(dim=1) + loss0 + loss1).mean()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "99a0Eh3AK5wT"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models zoo"
      ],
      "metadata": {
        "id": "ZHSuQd99s_Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architectures and loss functions"
      ],
      "metadata": {
        "id": "T0QP_le1tPL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_arch_and_loss(hparams):\n",
        "#     \"\"\"\n",
        "#     Returns the architecture and loss function based on the provided hparams.\n",
        "\n",
        "#     Args:\n",
        "#         hparams (dict): Hyperparameters dictionary, including 'ARCHITECTURE' and 'criterion'.\n",
        "\n",
        "#     Returns:\n",
        "#         arch: The model architecture.\n",
        "#         loss: The loss function.\n",
        "#     \"\"\"\n",
        "#     # Determine the number of outputs based on the loss function\n",
        "#     if hparams['criterion'] in ['B', 'N']:\n",
        "#         n_outputs = hparams['n_classes'] + 1  # Add 1 output neuron for BLoss or NLoss\n",
        "#     else:\n",
        "#         n_outputs = hparams['n_classes']  # Default number of outputs\n",
        "\n",
        "#     # Define the architectures\n",
        "#     architectures = {\n",
        "#         'CNN': CNN(n_outputs=n_outputs),\n",
        "#         'ResNet50': ResNet50(n_outputs=n_outputs, freeze=hparams.get('freeze', True)),\n",
        "#         'ViT': ViT(n_outputs=n_outputs, freeze=hparams.get('freeze', True)),\n",
        "#     }\n",
        "\n",
        "#     # Define the loss functions\n",
        "#     losses = {\n",
        "#         'CE':CELoss(),\n",
        "#         'B': BLoss(),\n",
        "#         'N': NLoss(),\n",
        "#     }\n",
        "\n",
        "#     # Get the architecture and loss based on hparams\n",
        "#     arch = architectures.get(hparams['architecture'])\n",
        "#     loss = losses.get(hparams['criterion'])\n",
        "\n",
        "#     if arch is None:\n",
        "#         raise ValueError(f\"Architecture '{hparams['ARCHITECTURE']}' is not supported.\")\n",
        "#     if loss is None:\n",
        "#         raise ValueError(f\"Loss function '{hparams['criterion']}' is not supported.\")\n",
        "\n",
        "#     return arch, loss\n"
      ],
      "metadata": {
        "id": "WfLgyavidUcW"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "CWrc-f3sWDDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def metrics(dataloader,model,hparams=hparams,loss_fn_red=None):\n",
        "#     # Collect images, predictions, and losses\n",
        "#     # images = []\n",
        "#     preds  = []\n",
        "#     labels = []\n",
        "#     losses = []\n",
        "#     correct= 0\n",
        "#     total  = 0\n",
        "#     for batch in dataloader:\n",
        "#         x, y, _ = batch\n",
        "#         with torch.no_grad():\n",
        "#             logits = model(x)\n",
        "#             # loss = loss_fn_red(h,y)\n",
        "#             pred = torch.argmax(logits[:,:hparams['n_classes']], dim=1)\n",
        "#         correct += (pred == y).sum().item()  # Number of correct predictions\n",
        "#         total += y.size(0)  # Total number of samples\n",
        "\n",
        "#         # images.extend(x.cpu())\n",
        "#         preds.extend(pred.cpu().numpy())\n",
        "#         labels.extend(y.cpu().numpy())\n",
        "#         # losses.extend(loss.cpu().numpy())\n",
        "#     acc = correct / total\n",
        "#     return preds, labels, acc"
      ],
      "metadata": {
        "id": "7793mMFKWFYS"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with LOSO Cross-Validation"
      ],
      "metadata": {
        "id": "KemJG62D7EJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset and Data Loaders"
      ],
      "metadata": {
        "id": "7d0C987gzzlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of the dataset, the dataloader, and the training module"
      ],
      "metadata": {
        "id": "tQmBPmYH21V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with NaN handling\n",
        "merged_data = load_and_preprocess_data(data_dir)\n",
        "\n",
        "# Verify data existence\n",
        "if not merged_data:\n",
        "    raise ValueError(\"No valid data found in directory!\")\n",
        "\n",
        "# Check available volunteers\n",
        "print(f\"Available volunteers: {list(merged_data.keys())}\")\n",
        "\n",
        "# Proceed with model training..."
      ],
      "metadata": {
        "id": "jrjw6mU-z36-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61700362-12c4-4c38-d18a-834a00558eaf"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using common labels: ['N2', 'R']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-250-2ae754bca9ea>:80: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in divide\n",
            "\n",
            "<ipython-input-250-2ae754bca9ea>:80: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in divide\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available volunteers: ['Vol_02', 'Vol_03', 'Vol_01']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-250-2ae754bca9ea>:80: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in divide\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "oBfy6DZL0tii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ClearML task\n",
        "task = Task.init(\n",
        "    project_name=\"ICML-2025\",\n",
        "    task_name=f\"arch_{ARCHITECTURE}_loss_{LOSS_FUN}\"\n",
        ")\n",
        "\n",
        "# Log hyperparameters\n",
        "task.connect({\n",
        "    \"architecture\": ARCHITECTURE,\n",
        "    \"loss_function\": LOSS_FUN,\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgJSo4xoe2cb",
        "outputId": "26820a3a-8d75-463c-85ae-03da4e66c1c7"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClearML Task: created new task id=d1eec303ee994a83a60f6fc16b959aa3\n",
            "ClearML results page: https://app.clear.ml/projects/ccaa059e6de442b6abe578eab9e214c8/experiments/d1eec303ee994a83a60f6fc16b959aa3/output/log\n",
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'architecture': 'SleepStageCNN', 'loss_function': 'CE'}"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize components\n",
        "datamodule = SleepDataModule(merged_data)\n",
        "model = SleepClassifier()\n",
        "\n",
        "# Setup data\n",
        "datamodule.setup()\n",
        "\n",
        "# Track LOSO accuracy\n",
        "loso_accuracies = []\n",
        "\n",
        "# LOSO Cross-Validation\n",
        "for test_subject in datamodule.subjects:\n",
        "    print(f\"\\n=== Training on {len(datamodule.subjects)-1} subjects, validating on {test_subject} ===\")\n",
        "\n",
        "    # Get data loaders for this split\n",
        "    train_loader, val_loader = datamodule.get_loso_splits(test_subject)\n",
        "\n",
        "    # Configure trainer\n",
        "    trainer = Trainer(\n",
        "        max_epochs=50,\n",
        "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n",
        "        enable_checkpointing=False\n",
        "    )\n",
        "\n",
        "    # Train/validate\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    # Get validation accuracy for this subject\n",
        "    val_acc = trainer.callback_metrics[\"val_acc\"].item()\n",
        "    loso_accuracies.append(val_acc)\n",
        "\n",
        "    # Log validation performance\n",
        "    task.get_logger().report_scalar(\n",
        "        title=\"Validation Accuracy\",\n",
        "        series=test_subject,\n",
        "        value=val_acc,\n",
        "        iteration=trainer.current_epoch\n",
        "    )\n",
        "\n",
        "    # Update task title with validation info\n",
        "    task.set_name(\n",
        "        f\"arch_{ARCHITECTURE}_loss_{LOSS_FUN}_seed_{seed}_noise_{NOISE_TYPE}_val_acc_{val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "# Compute final LOSO accuracy\n",
        "final_loso_accuracy = np.mean(loso_accuracies)\n",
        "print(f\"\\n=== Final LOSO Accuracy: {final_loso_accuracy:.4f} ===\")\n",
        "\n",
        "# Log final LOSO accuracy\n",
        "task.get_logger().report_scalar(\n",
        "    title=\"LOSO Accuracy\",\n",
        "    series=\"Final\",\n",
        "    value=final_loso_accuracy,\n",
        "    iteration=0\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JMvTOCCIN4hi",
        "outputId": "9043535a-59dd-447a-ea91-ec56aeb39940"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training on 2 subjects, validating on Vol_02 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MisconfigurationException",
          "evalue": "No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-262-101d4fcbfdc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Train/validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Get validation accuracy for this subject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         )\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach_model_logging_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         \u001b[0m_verify_loop_configurations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/configuration_validator.py\u001b[0m in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected: Trainer state fn must be set before validating loop configuration.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFITTING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0m__verify_train_val_loop_configuration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0m__verify_manual_optimization_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrainerFn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALIDATING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/configuration_validator.py\u001b[0m in \u001b[0;36m__verify_train_val_loop_configuration\u001b[0;34m(trainer, model)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mhas_optimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_overridden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"configure_optimizers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_optimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         raise MisconfigurationException(\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;34m\"No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;34m\" `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMisconfigurationException\u001b[0m: No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix and classification report\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for test_subject in datamodule.subjects:\n",
        "    val_loader = datamodule.get_loso_splits(test_subject)[1]  # Get validation loader\n",
        "    preds = []\n",
        "    labels = []\n",
        "    for batch in val_loader:\n",
        "        x, y = batch\n",
        "        with torch.no_grad():\n",
        "            y_hat = model(x)\n",
        "            preds.extend(torch.argmax(y_hat, dim=1).cpu().numpy())\n",
        "            labels.extend(y.cpu().numpy())\n",
        "\n",
        "    all_preds.extend(preds)\n",
        "    all_labels.extend(labels)\n",
        "\n",
        "# Log confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "task.get_logger().report_matrix(\n",
        "    title=\"Confusion Matrix\",\n",
        "    series=\"Final\",\n",
        "    matrix=cm,\n",
        "    iteration=0\n",
        ")\n",
        "\n",
        "# Log classification report\n",
        "report = classification_report(all_labels, all_preds, target_names=[\"NREM1\", \"NREM2\", \"REM\", \"Wake\"])\n",
        "task.get_logger().report_text(\n",
        "    title=\"Classification Report\",\n",
        "    series=\"Final\",\n",
        "    value=report\n",
        ")"
      ],
      "metadata": {
        "id": "CGGC14Fagbtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XHQ4Ays3gbjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store predictions from each model\n",
        "# all_predictions = []"
      ],
      "metadata": {
        "id": "4qZzt5w4smrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for seed in SEEDS:\n",
        "#     # Set seed for reproducibility at the VERY BEGINNING\n",
        "#     pl.seed_everything(seed)\n",
        "\n",
        "#     # Reinitialize the model architecture for each seed\n",
        "#     arch, loss_fn = get_arch_and_loss(hparams)\n",
        "#     # archs_and_losses = get_arch_and_loss(hparams)\n",
        "\n",
        "#     # arch, loss_fn = archs_and_losses[hparams['criterion']]['arch']\n",
        "#     # loss_fn = archs_and_losses[hparams['criterion']]['loss']\n",
        "\n",
        "\n",
        "#     checkpoint_callback_img = ModelCheckpoint(\n",
        "#         monitor='val_loss',       # Monitor validation loss\n",
        "#         dirpath=CHECKPOINT_PATH,  # Directory to save checkpoints\n",
        "#         filename=f'best_model_{ARCHITECTURE}_{LOSS_FUN}_{seed}_{NOISE_TYPE}',  # Checkpoint filename\n",
        "#         save_top_k=1,             # Save only the best model\n",
        "#         mode='min',               # Minimize validation loss\n",
        "#     )\n",
        "\n",
        "#     task = Task.init(project_name=\"ICML-2025\",\n",
        "#                      task_name=f'arch_{ARCHITECTURE}_loss_{LOSS_FUN}_seed_{seed}_noise_{NOISE_TYPE}')\n",
        "\n",
        "#     # Initialize the model with the reinitialized architecture\n",
        "#     model = train_model(model=arch, loss=loss_fn)\n",
        "\n",
        "#     # Log hyperparameters to ClearML\n",
        "#     task.connect(model.hparams)\n",
        "\n",
        "#     trainer = Trainer(max_epochs=hparams['num_epochs'],\n",
        "#                       callbacks=[checkpoint_callback_img],\n",
        "#                       accelerator=\"auto\", devices=\"auto\")\n",
        "#     trainer.fit(model, data_module)\n",
        "\n",
        "#     best_model_path = checkpoint_callback_img.best_model_path\n",
        "#     task.update_output_model(model_path=best_model_path, auto_delete_file=False)\n",
        "#     best_model = train_model.load_from_checkpoint(best_model_path,\n",
        "#                                                   model=arch,\n",
        "#                                                   loss=loss_fn)\n",
        "\n",
        "#     # Test set\n",
        "#     test_dataloader = data_module.test_dataloader()\n",
        "#     # Move the model to the correct device\n",
        "#     best_model = best_model.to(device)\n",
        "#     predictions = []\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_dataloader:\n",
        "#             x, _, _, = batch  # We only need the input data, not the labels\n",
        "#             logits = best_model(x.to(device))\n",
        "#             preds = torch.argmax(logits[:, :NUM_CLASSES], dim=1)\n",
        "#             predictions.append(preds.cpu().numpy())\n",
        "#     predictions = np.concatenate(predictions)  # Combine all batch predictions\n",
        "#     all_predictions.append(predictions)\n",
        "\n",
        "#     if seed != SEEDS[-1]:\n",
        "#         task.close()\n",
        "#         del[model, best_model, task, arch, loss_fn]"
      ],
      "metadata": {
        "id": "QisO7wDarFVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the models and the ensemble of the models"
      ],
      "metadata": {
        "id": "NYbuTthVuCDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_predictions"
      ],
      "metadata": {
        "id": "zOYyF657_kOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual models"
      ],
      "metadata": {
        "id": "yRxEnWceGYxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # List to store individual model accuracies\n",
        "# individual_accuracies = []\n",
        "\n",
        "# # Compute accuracy for each model\n",
        "# for i, predictions in enumerate(all_predictions):\n",
        "#     # Get predictions for the current model\n",
        "#     model_predictions = predictions  # Shape: (num_samples,)\n",
        "\n",
        "#     # Get true labels (already collected earlier)\n",
        "#     true_labels = np.array(data_module.cifar10_test.targets)\n",
        "\n",
        "#     # Calculate accuracy for the current model\n",
        "#     accuracy = accuracy_score(true_labels, model_predictions)\n",
        "#     individual_accuracies.append(accuracy)\n",
        "#     print(f'Model {i+1} Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# # Convert to numpy array for easier calculations\n",
        "# individual_accuracies = np.array(individual_accuracies)\n",
        "\n",
        "# # Compute mean accuracy\n",
        "# mean_accuracy = np.mean(individual_accuracies)\n",
        "\n",
        "# # Compute standard deviation of accuracy\n",
        "# std_accuracy = np.std(individual_accuracies)\n",
        "\n",
        "# print(f'Mean Accuracy: {mean_accuracy:.4f}')\n",
        "# print(f'Standard Deviation of Accuracy: {std_accuracy:.4f}')"
      ],
      "metadata": {
        "id": "WSr0zvfIGeSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble"
      ],
      "metadata": {
        "id": "iop73XibGhUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Stack predictions from all models\n",
        "# all_predictions = np.stack(all_predictions)  # Shape: (num_models, num_samples, num_classes)\n",
        "\n",
        "# # Ensemble predictions (e.g., by averaging)\n",
        "# ensemble_predictions = np.mean(all_predictions, axis=0)  # Shape: (num_samples, num_classes)\n",
        "# final_predictions, _ = mode(all_predictions, axis=0)  # Majority voting\n",
        "# final_predictions = final_predictions.flatten()  # Flatten to 1D array\n",
        "\n",
        "# # Get true labels from the CIFAR-10 data set\n",
        "# test_labels = np.array(data_module.cifar10_test.targets)\n",
        "# # test_labels = data_module.test_dataset.labels  # Adjust this based on your dataset\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(test_labels, final_predictions)\n",
        "# print(f'Ensemble Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# # Compute confusion matrix\n",
        "# cm = confusion_matrix(test_labels, final_predictions)"
      ],
      "metadata": {
        "id": "xyUFGexws8QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Simulated test metrics\n",
        "# test_metrics = {\n",
        "#     \"Mean Accuracy (individual)\": mean_accuracy,\n",
        "#     \"Standard Deviation of Accuracy (individual)\": std_accuracy,\n",
        "#     \"Ensemble Accuracy\": accuracy,\n",
        "# }\n",
        "\n",
        "# task.connect(test_metrics)"
      ],
      "metadata": {
        "id": "VbEGs97IgtRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task.close()"
      ],
      "metadata": {
        "id": "nIqc-cspappI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}